
%--------------------
\chapter{Modelo lineal dinámico}

Los modelos lineales dinámicos se pueden considerar como un caso particular de los modelos de estado espacio, y la estimación y el pronóstico para estos modelos se puede llevar a cabo usando el filtro de Kalman

\section{Representación de estado espacio y filtro de Kalman}
\subsection{Modelo de estado espacio}
Siguiendo la notación de \citeasnoun{Harvey1989}, la representación de estado espacio asume que en el tiempo $t$ el vector de una serie de tiempo multivariado de dimensión $N\times 1$ está relacionado con un vector $\bbeta_t$ (llamado vector de estado) de dimensión $m\times 1$ de la siguiente forma:
\begin{equation}\label{ec_obs}
\mathbf{y}_t=\mathbf{Z}_t\bbeta_t+\mathbf{d}_t+\boldsymbol{\epsilon}_t
\end{equation}

para $t=1,\cdots,T$, $\mathbf{Z}_t$ es una matriz de dimensión $N\times m$, $\mathbf{d}_t$ es un vector de dimensión $N\times 1$ y $\boldsymbol{\epsilon}_t$ es un vector de ruidos de dimensión $N\times 1$ con $E(\boldsymbol{\epsilon}_t)=\mathbf{0}$, $Var(\boldsymbol{\epsilon}_t)=\Sigma_t$ y $Cov(\boldsymbol{\epsilon}_t,\boldsymbol{\epsilon}_{t'})=\mathbf{0}$ para $t\neq t'$. La anterior ecuación se conoce como la ecuación de observación. 

En muchos casos, el vector $\bbeta_t$ contiene componentes no observables, y sigue la siguiente ecuación
\begin{equation}\label{ec_estado}
\bbeta_t=\mathbf{G}_t\bbeta_{t-1}+\mathbf{P}_t+\mathbf{R}_t\boldsymbol{\omega}_t
\end{equation}

donde $\mathbf{G}_t$ es una matriz de dimensión $m\times m$, $\mathbf{P}_t$ es un vector de dimensión $m\times 1$, $\mathbf{R}_t$ es una matriz de dimensión $m\times g$, $\boldsymbol{\omega}_t$ es un vector de ruidos de dimensión $g\times 1$ con $E(\boldsymbol{\omega}_t)=\mathbf{0}$, $Var(\boldsymbol{\omega}_t)=\mathbf{W}_t$ y $Cov(\boldsymbol{\omega}_t,\boldsymbol{\omega}_{t'})=\mathbf{0}$ para $t\neq t'$. La anterior ecuación se conoce como la ecuación de transición o ecuación de estado pues describe la evolución del vector de estado $\bbeta_t$. Se asume las siguientes propiedades para el vector de estado inicial: $E(\bbeta_0)=\mathbf{b}_0$ y $Var(\bbeta_0)=\mathbf{P}_0$. Finalmente se tiene que $Cov(\boldsymbol{\epsilon}_t,\ \boldsymbol{\omega}_{t'})=\mathbf{0}$ para todo $t$ y $t'$, $Cov(\boldsymbol{\epsilon}_t,\ \balpha_{0})=\mathbf{0}$ y $Cov(\boldsymbol{\omega}_t,\ \balpha_{0})=\mathbf{0}$  para todo $t$.

Un gran número de modelos estacionarios y no estacionarios puede ser escritos en forma de modelo de estado-espacio, a continuación se ilustran algunos más representativos. 

\begin{Eje}\label{AR_p}
\textbf{Modelos autorregresivos}. En general, un modelo autoregresivo $AR(p)$  dado por $y_t=\phi_0\phi_1y_{t-1}+\phi_2y_{t-2}+\cdots+\phi_py_{t-p}+e_t$, se puede escribir como un modelo lineal dinámico. , al definir $\bbeta_t=(
y_t,y_{t-1},\cdots,y_{t-p+1})'$, vector de dimensión $p\times 1$, se tiene que
\begin{equation*}
y_t=\underbrace{\begin{pmatrix}
1&0&\cdots&0
\end{pmatrix}}_{\mathbf{Z}_t}\underbrace{\begin{pmatrix}
y_t\\y_{t-1}\\\vdots\\y_{t-p+1}
\end{pmatrix}}_{\bbeta_t}+\underbrace{\phi_0}_{\mathbf{d}_t},
\end{equation*}

\begin{equation*}
\underbrace{\begin{pmatrix}
y_t\\y_{t-1}\\\vdots\\y_{t-p+1}
\end{pmatrix}}_{\bbeta_t}=\underbrace{\begin{pmatrix}
\phi_1&\phi_2&\cdots&\phi_{p-1}&\phi_p\\
1&0&\cdots&0&0\\
0&1&\cdots&0&0\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\cdots&1&0
\end{pmatrix}}_{\mathbf{G}_t}\underbrace{\begin{pmatrix}
y_{t-1}\\y_{t-2}\\\vdots\\y_{t-p}
\end{pmatrix}}_{\bbeta_{t-1}}+\underbrace{\begin{pmatrix}
1\\0\\\vdots\\0
\end{pmatrix}}_{\mathbf{R}_t}\underbrace{e_t}_{\boldsymbol{\omega}_t}
\end{equation*}

Nótese que en la anterior representación de modelo de estado-espacio, las matrices $\mathbf{Z}_t$, $\mathbf{d}_t$ $\mathbf{G}_t$ y $\mathbf{R}_t$ son matrices constantes que no dependen del tiempo, lo anterior se conoce como un modelo invariante en el tiempo.
\end{Eje}

\begin{Eje}
\textbf{Medias móviles}. Un modelo de medias móviles de órden $q$, $MA(q)$, se puede escribir como $y_t=\mu+e_t+\theta_1e_{t-1}+\cdots+\theta_qe_{t-q}$ definiendo el vector de estado como $\bbeta_t=(y_t,e_t,e_{t-1},\cdots,e_{t-q+1})'$. La ecuación de observación se establece con $\mathbf{Z}_t=(1,0,\cdots,0)$, $\mathbf{d}_t=\mu$. La ecuación de transición queda expresada como
\begin{equation*}
\underbrace{\begin{pmatrix}
y_t\\\theta_1e_{t}\\\theta_2e_{t-1}\\\vdots\\\theta_qe_{t-q+1}
\end{pmatrix}}_{\bbeta_t}=\underbrace{\begin{pmatrix}
0&1&\cdots&1&1&1\\
0&0&\cdots&0&0&0\\
0&\frac{\theta_1}{\theta_2}&\cdots&0&0&0\\
\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\
0&0&\cdots&0&\frac{\theta_{q-1}}{\theta_q}&0
\end{pmatrix}}_{\mathbf{G}_t}\underbrace{\begin{pmatrix}
y_{t-1}\\\theta_1e_{t-1}\\\theta_2e_{t-2}\\\vdots\\\theta_qe_{t-q}
\end{pmatrix}}_{\bbeta_{t-1}}+\underbrace{\begin{pmatrix}
1\\\theta_1\\0\\\vdots\\0
\end{pmatrix}}_{\mathbf{R}_t}\underbrace{e_t}_{\boldsymbol{\omega}_t}
\end{equation*}
\end{Eje}

La representación en modelo de estado-espacio es particularmente útil para los modelos estructurales. A continuación se ilustra la representación en forma de estado-espacio de uno de estos modelos

\begin{Eje}\label{ModEstBas}
\textbf{Modelo estructural básico} Este modelo asume que la serie observada $y_t$ es la suma de un componente de nivel $\mu_t$, un componente estacional $\gamma_t$ y un componente del ruido. El modelo está dado por:
\begin{align*}
y_t&=\mu_t+\gamma_t+\epsilon_t\\
\mu_t&=\mu_{t-1}+\beta_{t-1}+\eta_t\\
\beta_t&=\beta_{t-1}+\psi_t
\end{align*}

donde $\mu_t$ denota el componente de la tendencia, $\{\epsilon_t\}$ es un proceso ruido blanco, y $\gamma_t$ es el componente de estascionalidad que cumple $\sum_{i=0}^{s-1}\gamma_{t-i}=\omega_t$ donde $s$ denota el número de observaciones por año, y $\{\omega_t\}$ es un proceso ruido blanco. 

El vector de estado contiene todos los componentes no observables del modelo: el nivel $\mu_t$, la pendiente $\beta_t$ y $s-1$ efectos estacionales, de esta forma $\bbeta_t=(\mu_t,\beta_t,\gamma_t,\gamma_{t-1},\cdots,\gamma_{t-s+1})'$. La ecuación de observación y de estado quedan definidas como
\begin{align*}
y_t&=(1,0,1,0,\cdots,0)'\bbeta_t+\epsilon_t\\
\bbeta_t&=\begin{pmatrix}\mu_t\\\beta_t\\\gamma_t\\\gamma_{t-1}\\\vdots\\\gamma_{t-s+3}\\\gamma_{t-s+2}\end{pmatrix}
=\begin{pmatrix}1&1&0&0&\cdots&0&0\\
0&1&0&0&\cdots&0&0\\
0&0&-1&-1&\cdots&-1&-1\\
0&0&1&0&\cdots&0&0\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&0&\cdots&1&0
\end{pmatrix}
\begin{pmatrix}\mu_{t-1}\\\beta_{t-1}\\\gamma_{t-1}\\\gamma_{{t-2}}\\\vdots\\\gamma_{t-s+2}\\\gamma_{t-s+1}\end{pmatrix}+
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1\\
0&0&0\\
0&0&0\\
\vdots&\vdots&\vdots\\
0&0&0\\
\end{pmatrix}
\begin{pmatrix}\eta_t\\\psi_t\\\omega_t\end{pmatrix}
\end{align*}
\end{Eje}

\subsection{Filtro de Kalman}
Tal como se vió en los ejemplos de la sección anterior, en algunos modelos de estado espacio el vector de estado $\bbeta_t$ contiene componentes no observables de la serie $y_t$, y es de interés extraer (estimar) estos componentes, esto es particularmente cierto en los modelos estructurales. El filtro de Kalman permite estimar estos componentes usando la serie observada $y_t$.

En otros modelos de estado espacio, es de interés estimar los elementos de las matrices $\mathbf{Z}_t$, $\mathbf{d}_t$, $\mathbf{G}_t$ o $\mathbf{P}_t$, por ejemplo, en el ejemplo \ref{AR_p} donde se encuentra una representación en modelo de estado espacio de los modelos autoregresivos, el interés radica en estimar $\mathbf{d}_t$ y $\mathbf{G}_t$ ya que estos contienen los coeficientes $\phi_0,\phi_1,\cdots,\phi_p$ del modelo autoregresivo; adicionalmente también es de interés estimar la varianza del ruido $\omega_t$. La estimación de estos coeficientes se lleva a cabo usando el método de máxima verosimilitud, y se efectúa también por medio del filtro de Kalman.

\subsubsection{Las dos fases del filtro de Kalman}
El filtro de Kalman consta de dos fases: el fase de filtro y el fase de suavizamiento:
\begin{itemize}
\item \emph{Fase de filtro}: Esta fase tiene objetivo encontrar el óptimo estimador del vector de estado en el tiempo $t$, $\bbeta_t$ con base en la información disponible hasta ese punto de tiempo: $y_1,y_2,\cdots,y_t$.
\item \emph{Fase de suavizamiento}: Esta fase se ejecuta después de la fase de filtro, y tiene objetivo encontrar el óptimo estimador de $\bbeta_t$ con base en toda la información disponible: $y_1,y_2,\cdots,y_n$.
\end{itemize}

Denotando $\mathbf{y}^t$ como toda la información disponible hasta el tiempo $t$, es decir, $\mathbf{y}_1,\mathbf{y}_2,\cdots,\mathbf{y}_t$, se define el óptimo estimador de $\bbeta_t$ como $\mathbf{m}_t$ el cual está dado por $E(\bbeta_t\mid y^t)$, y se denota la matriz de covarianzas del error de estimación como $\mathbf{P}_t=E[(\bbeta_t-\mathbf{m}_t)(\bbeta_t-\mathbf{m}_t)']$. Se tiene las siguientes ecuaciones recursivas (conocidas como ecuaciones de actualización):
\begin{align}
\mathbf{m}_t&=\mathbf{m}_{t|t-1}+\mathbf{P}_{t|t-1}\mathbf{Z}'_t\mathbf{F}^{-1}_t(\mathbf{y}_t-\mathbf{Z}_t\mathbf{m}_{t|t-1}-\mathbf{d}_t)\label{actua_1}\\
\mathbf{P}_t&=\mathbf{P}_{t|t-1}-\mathbf{P}_{t|t-1}\mathbf{Z}_t'\mathbf{F}_t^{-1}\mathbf{Z}_t\mathbf{P}_{t|t-1}\label{actua_2}
\end{align}

donde $\mathbf{F}_t=\mathbf{Z}_t\mathbf{P}_{t|t-1}\mathbf{Z}'_t+\Sigma_t$, $\mathbf{m}_{t|t-1}=E(\bbeta_{t}|\mathbf{y}^{t-1})$ es el estimador óptimo de $\bbeta_t$ dada información hasta el tiempo $t-1$, y está dada por
\begin{equation}\label{actua_3}
\mathbf{m}_{t|t-1}=\mathbf{G}_{t}\mathbf{m}_{t-1}+\mathbf{P}_{t}
\end{equation}

y la matriz de covarianzas del error de estimación $\mathbf{P}_{t|t-1}=E[(\bbeta_t-\mathbf{m}_{t|t-1})(\bbeta_t-\mathbf{m}_{t|t-1})']$ está dada por
\begin{equation}\label{actua_4}
\mathbf{P}_{t|t-1}=\mathbf{G}_{t}\mathbf{P}_{t-1}\mathbf{G}_t'+\mathbf{R}_{t}\mathbf{W}_{t}\mathbf{R}_{t}'
\end{equation}

Asumiendo las condiciones iniciales $\mathbf{m}_0$ y $\mathbf{P}_0$, la fase de filtro se ejecuta de la siguiente forma:
\begin{enumerate}
\item Calcular $\mathbf{m}_{1|0}$ y $\mathbf{P}_{1|0}$ usando las condiciones iniciales $\mathbf{m}_0$ y $\mathbf{P}_0$ a partir de (\ref{actua_3}) y (\ref{actua_4}) 
\item Calcular $\mathbf{m}_{1}$ y $\mathbf{P}_{1}$ usando $\mathbf{m}_{1|0}$ y $\mathbf{P}_{1|0}$ a partir de (\ref{actua_1}) y (\ref{actua_2}) 
\item Calcular $\mathbf{m}_{2|1}$ y $\mathbf{P}_{2|1}$ usando las condiciones iniciales $\mathbf{m}_1$ y $\mathbf{P}_1$ a partir de (\ref{actua_3}) y (\ref{actua_4}) 
\item $\ \ \ \ \ \ \ \cdots$
\item Calcular $\mathbf{m}_{T}$ y $\mathbf{P}_{T}$ usando $\mathbf{m}_{T|T-1}$ y $\mathbf{P}_{T|T-1}$ a partir de (\ref{actua_1}) y (\ref{actua_2}) 
\end{enumerate}

Al finalizar el fase del filtro, se cuenta con las estimaciones $\mathbf{m}_1,\mathbf{m}_2,\cdots,\mathbf{m}_T$, sin embargo, la estimación de cada $\mathbf{m}_t$ solo se basa en la información disponible hasta el tiempo $t$, y no utilizando toda la información disponible, por lo cual se recurre a la fase de suavizamiento cuyo objetivo es encontrar los estimadores $\mathbf{m}_1^T,\mathbf{m}_2^T,\cdots,\mathbf{m}_T^T$ donde para cada $t$, $\mathbf{m}_t^T=E(\bbeta_t\mid\mathbf{y}^T)$. Las ecuaciones de esta fase están dadas por
\begin{equation}\label{Suavi_1}
\mathbf{m}_t^T=\mathbf{m}_t+\mathbf{P}_t^*(\mathbf{m}_{t+1}^T-\mathbf{G}_{t+1}\mathbf{m}_t)
\end{equation}

con la matriz de covarianzas del error de estimación dada por
\begin{equation}\label{Suavi_2}
\mathbf{P}_t^T=\mathbf{P}_t+\mathbf{P}_t^*(\mathbf{P}_{t+1}^T-\mathbf{P}_{t+1|t}(\mathbf{P}_t^*)')
\end{equation}

donde 
\begin{equation}\label{Suavi_3}
\mathbf{P}_t^*=\mathbf{P}_t\mathbf{G}_{t+1}'\mathbf{P}_{t+1|t}^{-1}
\end{equation}

donde $\mathbf{P}_{t+1|t}=\mathbf{G}_{t+1}\mathbf{P}_{t}\mathbf{G}_{t+1}'+\mathbf{R}_{t+1}\mathbf{W}_{t+1}\mathbf{R}_{t+1}'$. Observando la forma de las anteriores ecuaciones, se puede ver que la fase de suavizamiento se aplica comenzando en el tiempo $T-1$ y se va retrocediendo en tiempo hasta el tiempo $1$; para el tiempo $T$ no es necesario el cálculo puesto que $\mathbf{m}_T=\mathbf{m}_T^T$. En conclusión, una vez concluida la fase del filtro, la implementación de la fase de suavizamiento es como sigue:
\begin{enumerate}
\item Calcular $\mathbf{P}_{T-1}^*$ usando (\ref{Suavi_3})
\item Calcular $\mathbf{m}_{T-1}^T$ y $\mathbf{P}_{T-1}^T$ usando (\ref{Suavi_1}) y (\ref{Suavi_2})
\item Calcular $\mathbf{P}_{T-2}^*$ usando (\ref{Suavi_3})
\item Calcular $\mathbf{m}_{T-2}^T$ y $\mathbf{P}_{T-2}^T$ usando (\ref{Suavi_1}) y (\ref{Suavi_2})
\item $\ \ \ \ \ \ \ \cdots$
\item Calcular $\mathbf{P}_{1}^*$ usando (\ref{Suavi_3})
\item Calcular $\mathbf{m}_{1}^T$ y $\mathbf{P}_{1}^T$ usando (\ref{Suavi_1}) y (\ref{Suavi_2})
\end{enumerate}

Una vez concluida la fase de suavizamiento, se cuenta con el óptimo estimador del vector de estado $\bbeta_t$ dada toda la información disponible $\mathbf{y}^T$, y se puede extraer el componente de $\bbeta_t$ que sea de interés. En el ejemplo \ref{ModEstBas}, si el interés está en estimar la tendencia de la serie $y_t$, entonces de los resultados del filtro de Kalman, se extrae el primer componente de $\mathbf{m}_t^T$ que corresponde a la tendencia $\mu_t$, y si el interés es observar la evolución de la estacionalidad en el tiempo, se extrae los últimos $s$ componentes del vector $\mathbf{m}_t^T$ para $t=1,\cdots,T$.  

\subsubsection{Estimación de máxima verosimilitud}
Para la aplicación del filtro de Kalman es necesario conocer los parámetros de la ecuación de observación y la ecuación de estado, es decir, las matrices y los vectores $\mathbf{Z}_t$, $\mathbf{d}_t$, $\Sigma_t$, $\mathbf{G}_t$, $\mathbf{P}_t$ y $\mathbf{W}_t$. Para la estimación de estos parámetros en el ámbito de la inferencia clásica, se hace uso de la función de verosimilitud condicional dado por
\begin{equation*}
L(\mathbf{y},\boldsymbol{\theta})=\prod_{t=1}^Tp(\mathbf{y}_t\mid \mathbf{y}^{t-1})
\end{equation*}

donde $\mathbf{\theta}$ denota todos los parámetros del modelo. Si asumimos que la distribución del ruido de la ecuación de observación (\ref{ec_obs}) es normal multivariante, al igual que el vector de estado inicial, entonces se puede concluir que $\mathbf{y}_t\mid \mathbf{y}^{t-1}\sim N_N(\mathbf{Z}_t\mathbf{a}_{t\mid t-1}+\mathbf{d}_t,\ \mathbf{F}_t)$ con $\mathbf{F}_t=\mathbf{Z}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}_t'+\Sigma_t$. De esta forma, se tiene que
\begin{equation*}
p(\mathbf{y}_t\mid \mathbf{y}^{t-1})=(2\pi)^{-N/2}\mid F_t\mid^{-1/2}\exp\left\{-\frac{1}{2}\mathbf{v}_t'\mathbf{F}_t^{-1}\mathbf{v}_t\right\}
\end{equation*}

con $\mathbf{v}_t=\mathbf{y}_t-E(\mathbf{y}_t\mid\mathbf{y}^{t-1})=\mathbf{y}_t-\mathbf{Z}_t\mathbf{a}_{t\mid t-1}-\mathbf{d}_t$ que denota los errores de predicción en el tiempo $t$. 

Dado lo anterior, la función de verosimilitud condicional está dada por
\begin{equation*}
L(\mathbf{y},\mathbf{\theta})=(2\pi)^{-NT/2}\prod_{t=1}^T\mid F_t\mid^{-1/2}\exp\left\{-\frac{1}{2}\sum_{t=1}^T\mathbf{v}_t'\mathbf{F}_t^{-1}\mathbf{v}_t\right\}
\end{equation*}

y la función de log verosimilitud condicional está dada por:
\begin{equation*}
l(\mathbf{y},\mathbf{\theta})=-\frac{NT}{2}\log (2\pi)-\frac{1}{2}\sum_{t=1}^T\log\mid F_t\mid-\frac{1}{2}\sum_{t=1}^T\mathbf{v}_t'\mathbf{F}_t^{-1}\mathbf{v}_t
\end{equation*}

Recomendamos la lectura de la sección 3.4 de \citeasnoun{Harvey1989} para consultar los diferentes aspectos técnicos de la estimación de máxima verosimilitud. 

En cuanto a la implementación computacional del filtro de Kalman en \verb'R', se encuentran varios paquetes disponibles, entre ellos \verb'dse', \verb'dlm', \verb'FKF' y \verb'sspir'. En el artículo de \citeasnoun{Kalman_R} se encuentra una descripción detallada y comparación entre estos paquetes.

\section{Modelos bayesianos dinámicos}

Los modelos lineales dinámicos son un caso particular de los modelos de estado espacio lineal y Gaussiano. Adicionalmente, aquí se considera únicamente el caso de series univariadas. Por lo tanto, estos modelos quedan especificados por las dos siguientes ecuaciones,
\begin{align}
y_t&=\mathbf{Z}_t\bbeta_t+\epsilon_t,\ \ \ \ \epsilon_t\sim N(0,\sigma_t^2)\\
\bbeta_t&=\mathbf{G}_t\bbeta_{t-1}+\boldsymbol{\omega}_t,\ \ \ \ \omega_t\sim N_m(\mathbf{0},\mathbf{W}_t)
\end{align}

donde $\{y_t\}$ es una serie de tiempo observada univariada, condicionalmente independiente dado $\bbeta_t$ y $\sigma^2_t$, $\mathbf{Z}_t$ es un vector de variables aleatorias de dimensión $1\times m$, y $\bbeta_t$ es el vector de estado o el vector de coeficientes de regresión en el tiempo $t$ de dimensión $m\times1$, $\mathbf{G}_t$ es de dimensión $m\times m$. Los errores $\epsilon_t$ y $\boldsymbol{\omega}_t$ son independientes, con varianza $\sigma^2_t$ y $\mathbf{W}_t$, respectivamente. Adicionalmente se supone que $\bbeta_1\sim N_m(\mathbf{a}_1,\mathbf{R}_1)$. 

Algunos caso especiales de los modelos lineales dinámicos son los modelos de regresión dinámico que se dan cuando $\mathbf{G}_t=\mathbf{I}_m$ para todo $t$. Los modelos de regresión clásicos o estáticos también es un caso particular de los modelos lineales dinámicos con $\mathbf{G}_t=\mathbf{I}_m$ y $\mathbf{W}_t=0$ para todo $t$.

\begin{Eje}
Gamerman (2003) afirma que el modelo de crecimiento lineal dado por
\begin{align*}
y_t&=\beta_{1t}+\epsilon_t\ \ \ \ \epsilon_t\sim N(0,\sigma^2_t)\\
\beta_{1t}&=\beta_{1,t-1}+\beta_{2t}+\omega_{1t}\\
\beta_{2t}&=\beta_{2,t-1}+\omega_{2t}
\end{align*}

es un modelo lineal dinámico con $\bbeta_t=(\beta_{1t},\beta_{2t})'$, $\mathbf{Z}_t=(1,0)$, $\mathbf{G}_t=\begin{pmatrix}
1&1\\
1&0
\end{pmatrix}$ y $\boldsymbol{\omega}_t=(\omega_{1t},\omega_{2t})'\sim N_2(\mathbf{0},\mathbf{W}_t)$.
\end{Eje}

El objetivo del análisis es encontrar la distribución a posteriori de los coeficientes de regresión $\bbeta_t$ para todo $t=1,\ldots,T$. Esto se lleva a cabo usando procedimientos recursivos que constan de los procesos del filtro y del suavizamiento que se describen a continuación. Para la implementación de estos procesos, se supone que $\sigma^2_t$, $\mathbf{W}_t$, $\mathbf{F}_t$ y $\mathbf{G}_t$ son conocidas; de lo contrario, se debe incluir la estimación de estos parámetros en los procedimientos recursivos.

\subsection{Fase de filtro}
El proceso del filtro tiene como objetivo encontrar la distribución de $\bbeta_t$ dada toda la información $y_1,\ldots,y_t$ para todo $t=1,\ldots,T$, esto es, $p(\bbeta_t\mid y^t)$, donde $y^t$ denota el conjunto de información disponible hasta el tiempo $t$. Para eso, utilizamos el resultado de construcción de distribución conjunta de una distribución normal estándar tomada de \citeasnoun{Gamer06}, el cual afirma que:
\begin{Res}
Si $x_1\mid x_2\sim N_{d_1}(\mu_1+B_1(x_2-\mu_2),\ B_2)$ para matrices de constantes $B_1$ y $B_2$ de dimensión $d_1\times d_1$ y $d_2\times d_2$, respectivamente, y si $x_2\sim N_{d_2}(\mu_2,\ \Sigma_{22})$, entonces se tiene que 
\begin{equation*}
x=\begin{pmatrix}x_1\\x_2\end{pmatrix}\sim N_d\left[\begin{pmatrix}\mu_1\\\mu_2\end{pmatrix},\ \begin{pmatrix}\Sigma_{11}&\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{pmatrix}\right]
\end{equation*}

donde $\Sigma_{11}=B_2+B_1\Sigma_{22}B_1'$ y $\Sigma_{21}'=\Sigma_{12}=B_1\Sigma_{22}$.
\end{Res}

Se aplica el anterior resultado al tomar $\bbeta_{t} \mid y^{t-1}$  como $x_1$ y tomar $\bbeta_{t-1} \mid y^{t-1}$ como $x_2$, de donde se tiene que $x_1 \mid x_2=\bbeta_t \mid \bbeta_{t-1}$ cuya distribución es $N_p(\mathbf{G}_t\bbeta_{t-1},\mathbf{W}_t)$ (obtenida de la ecuación de estado), es decir, $\mathbf{G}_t\bbeta_{t-1}=\mu_1+B_1(x_2-\mu_2)$. Por otro lado, suponga que en el tiempo $t-1$ la distribución de $\bbeta_{t-1} \mid y^{t-1}$ es $N_m(\mathbf{m}_{t-1},\mathbf{P}_{t-1})$, de esta forma, $\mu_2=\mathbf{m}_{t-1}$ y $\Sigma_{22}=\mathbf{P}_{t-1}$. Ahora, teniendo en cuenta que $\mathbf{G}_t\bbeta_{t-1}=\mathbf{G}_t\mathbf{m}_{t-1}+\mathbf{G}_t(\bbeta_{t-1}-\mathbf{m}_{t-1})$, se puede concluir que $\mu_1=\mathbf{G}_t\mathbf{m}_{t-1}$ y $B_1=\mathbf{G}_t$, y del resultado anterior, se concluye que
\begin{equation}\label{t_t-1}
\bbeta_t \mid y^{t-1}\sim N_m(\mathbf{m}_{t\mid t-1},\mathbf{P}_{t\mid t-1})
\end{equation}

con $\mathbf{m}_{t\mid t-1}=\mathbf{G}_t\mathbf{m}_{t-1}$ y $\mathbf{P}_{t\mid t-1}=\mathbf{W_t}+\mathbf{G}_t\mathbf{P}_{t-1}\mathbf{G}_t'$.

Cuando se observa la observación $t$, se debe actualizar la distribución de $\bbeta_t$ calculando la distribución $p(\bbeta_t \mid y^t)$, tenemos que
\begin{align*}
p(\bbeta_t \mid y^t)&=p(\bbeta_t \mid y_t,y^{t-1})\\
&\propto p(y_t,y^{t-1},\bbeta_t)\\
&\propto p(y_t,y^{t-1},\bbeta_t)\frac{1}{p(y^{t-1})}\\
&=\frac{p(y_t,y^{t-1},\bbeta_t)}{p(\bbeta_t,y^{t-1})}\frac{p(\bbeta_t,y^{t-1})}{p(y^{t-1})}\\
&=p(y_t \mid \bbeta_t,y^{t-1})p(\bbeta_t \mid y^{t-1})\\
&=p(y_t \mid \bbeta_t)p(\bbeta_t \mid y^{t-1})
\end{align*}

puesto que dado $\bbeta_t$, la distribución de $y_t$ es independiente de informaciones previas. Ahora usando $y_t \mid \bbeta_t\sim N(\mathbf{Z}_t\bbeta_t,\sigma^2_t)$ y $\bbeta_t \mid y^{t-1}\sim N_m(\mathbf{m}_{t\mid t-1},\mathbf{P}_{t\mid t-1})$, se tiene que
\begin{align*}
p(\bbeta_t \mid y^t)&\propto \exp\left\{-\frac{1}{2\sigma_t^2}(y_t-\mathbf{Z}_t\bbeta_t)^2\right\}\exp\left\{-\frac{1}{2}(\bbeta_t-\mathbf{m}_{t\mid t-1})'\mathbf{P}_{t\mid t-1}^{-1}(\bbeta_t-\mathbf{m}_{t\mid t-1})\right\}\\
&\propto\exp\left\{-\frac{1}{2}\left[\bbeta_t'(\sigma^{-2}_t\mathbf{Z}'_t\mathbf{Z}_t+\mathbf{P}_{t\mid t-1}^{-1})\bbeta_t-2\bbeta_t'(\sigma^{-2}_t\mathbf{Z}'_ty_t+\mathbf{P}_{t\mid t-1}^{-1}\mathbf{m}_{t\mid t-1})\right]\right\}\\
&=\exp\left\{-\frac{1}{2}\left[\bbeta_t'\mathbf{P}_t^{-1}\bbeta_t-2\bbeta_t'\mathbf{P}_t^{-1}\mathbf{P}_t(\sigma^{-2}_t\mathbf{Z}'_ty_t+\mathbf{P}_{t\mid t-1}^{-1}\mathbf{m}_{t\mid t-1})\right]\right\}
\end{align*}

con $\mathbf{P}_t^{-1}=\sigma^{-2}_t\mathbf{Z}'_t\mathbf{Z}_t+\mathbf{P}_{t\mid t-1}^{-1}$. De lo anterior, se tiene que 
\begin{equation*}
\bbeta_t\mid y^t \sim N_m(\mathbf{m}_t,\ \mathbf{P}_t)
\end{equation*}

con $\mathbf{m}_t=\mathbf{P}_t(\sigma^{-2}_t\mathbf{Z}'_ty_t+\mathbf{P}_{t\mid t-1}^{-1}\mathbf{m}_{t\mid t-1})$. Gamerman (2003) afirma que 
\begin{equation}\label{demostrarP}
\mathbf{P}_t=\mathbf{P}_{t\mid t-1}-\mathbf{A}_t\mathbf{A}_t'\mathbf{Q}_t
\end{equation}

con $\mathbf{Q}_t=\mathbf{Z}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t+\sigma^2_t$ y $\mathbf{A}_t=\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t/\mathbf{Q}_t$. Entonces se tiene que
\begin{align*}
\mathbf{m}_t&:=\mathbf{P}_t(\sigma^{-2}_t\mathbf{F}_ty_t+\mathbf{P}_{t\mid t-1}^{-1}\mathbf{m}_{t\mid t-1})\\
&=\left\(\mathbf{P}_{t\mid t-1}-\frac{\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t\mathbf{Z}_t\mathbf{P}_{t\mid t-1}}{\mathbf{Q}_t}\right\)\left\(\sigma^{-2}_t\mathbf{Z}'_ty_t+\mathbf{P}_{t\mid t-1}^{-1}\mathbf{m}_{t\mid t-1}\right\)\\
&=\mathbf{m}_{t\mid t-1}+\sigma^{-2}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}'_ty_t-\frac{\sigma^{-2}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t\mathbf{Z}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}'_ty_t}{\mathbf{Q}_t}-\frac{\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t\mathbf{Z}_t\mathbf{m}_{t\mid t-1}}{\mathbf{Q}_t}\\
&=\mathbf{m}_{t\mid t-1}+\frac{\mathbf{P}_{t\mid t-1}\mathbf{Z}'_t}{\mathbf{Q}_t}(\sigma^{-2}_ty_t\mathbf{Q}_t-\sigma^{-2}_t\mathbf{Z}_t\mathbf{P}_{t\mid t-1}\mathbf{Z}'_ty_t-\mathbf{Z}_t\mathbf{m}_{t\mid t-1})\\
&=\mathbf{m}_{t\mid t-1}+\mathbf{A}_t(y_t-\mathbf{Z}_t\mathbf{m}_{t\mid t-1}).
\end{align*}

De esta forma se tiene que
\begin{equation*}
p(\bbeta_t \mid y^t)\propto \exp\left\{-\frac{1}{2}\left[\bbeta_t'\mathbf{P}_t^{-1}\bbeta_t-2\bbeta_t'\mathbf{P}_t^{-1}\mathbf{m_t}\right]\right\},
\end{equation*}
esto es,
\begin{equation}\label{tt}
\bbeta_t \mid y^t\sim N_p(\mathbf{m_t},\mathbf{P}_t).
\end{equation}


En conclusión, el proceso del filtro consta de
\begin{enumerate}
    \item Fijar una distribución a priori $\bbeta_1 \mid y^0\sim N_p(\mathbf{a}_1,\mathbf{R}_1)$
    \item Obtener la distribución actualizada $\bbeta_1 \mid y^1$ usando (\ref{tt})
    \item Calcular la distribución de $\bbeta_2 \mid y^1$ usando (\ref{t_t-1})
    \item Calcular la distribución de $\bbeta_2 \mid y^2$ usando (\ref{tt})
    \item $\ldots$
    \item Obtener la distribución de $\bbeta_n \mid y^n$ usando (\ref{tt}).
\end{enumerate}


\subsection{Fase de suavizamiento}
Una vez concluido el algoritmo del filtro, se procede con el suavizamiento que tiene como fin encontrar la distribución de $\beta_t$ dada toda la información $y_1,\ldots,y_n$ para todo $t=1,\ldots,n$.

En primer lugar, se encuentra la distribución conjunta de las observaciones $y_t$ y los coeficientes de regresión $\bbeta_t$ para $t=1,\ldots,n$.

\begin{Res}
La función de densidad de la distribución conjunta de $\mathbf{y}^n=(y_1,\ldots,y_n)'$ y $\bbeta=(\bbeta_1,\ldots,\bbeta_n)'$ está dada por
\begin{equation}
p(\mathbf{y}^n,\bbeta)=\prod_{t=1}^np(\mathbf{y}_t \mid \bbeta_t)\prod_{t=2}^np(\bbeta_t \mid \bbeta_{t-1})p(\bbeta_1).
\end{equation}
\end{Res}

\begin{proof}
Tenemos que
\begin{equation*}
p(y^n,\bbeta)=p(y_1,\ldots,y_n \mid \bbeta_1,\ldots,\bbeta_n)p(\bbeta_1,\ldots,\bbeta_n).
\end{equation*}
Ahora
\begin{align*}
&\ \ \ \ \ p(y_1,\ldots,y_n \mid \bbeta_1,\ldots,\bbeta_n)\\
&=p(y_1 \mid y_2,\ldots,y_n,\bbeta_1,\ldots,\bbeta_n)p(y_2 \mid y_3,\ldots,y_n,\bbeta_1,\ldots,\bbeta_n)\ldots p(y_n \mid \bbeta_1,\ldots,\bbeta_n)\\
&=p(y_1 \mid \bbeta_1)p(y_2 \mid \bbeta_2)\ldots p(y_n \mid \bbeta_n),
\end{align*}
puesto que para todo $i=1,\ldots,n$, dado $\bbeta_i$, $y_i$ es independiente de $y_j$ para todo $j>i$.

Por otro lado
\begin{align*}
&\ \ \ \ \ p(\bbeta_1,\ldots,\bbeta_n)\\
&=p(\bbeta_n \mid \bbeta_{n-1},\ldots,\bbeta_1)p(\bbeta_{n-1} \mid \bbeta_{n-2},\ldots,\bbeta_1)\ldots p(\bbeta_2 \mid \bbeta_1)p(\bbeta_1)\\
&=p(\bbeta_n \mid \bbeta_{n-1})p(\bbeta_{n-1} \mid \bbeta_{n-2})\ldots p(\bbeta_2 \mid \bbeta_1)p(\bbeta_1),
\end{align*}
puesto que para todo $i=1,\ldots,n-1$, dado $\bbeta_i$, $\bbeta_{i+1}$ es independiente de $\bbeta_j$ para todo $j<i+1$.
\end{proof}
Usando la anterior función de densidad conjunto, se puede encontrar la distribución posteriori de $\bbeta_t$ dada toda la información $y_1,\ldots,y_n$ para todo $t=1,\ldots,n$, como lo ilustra el siguiente resultado.
\begin{Res}
Dado $y_1,\ldots,y_n$, $\bbeta_1$, $\ldots$, $\bbeta_{t-1}$, $\bbeta_{t+1}$, $\ldots$, $\bbeta_n$, la distribución posteriori de $\bbeta_t$ es $N_p(\mathbf{b}_t,\mathbf{B}_t)$ con
\begin{equation*}
\mathbf{b}_t=
\begin{cases}
\mathbf{B}_1(\sigma^{-2}_1\mathbf{F}_1y_1+\mathbf{G}_2'\mathbf{W}_2^{-1}\bbeta_2+\mathbf{R}_1^{-1}\mathbf{a}_1)&\ t=1\\
\mathbf{B}_t(\sigma^{-2}_t\mathbf{F}_ty_t+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\bbeta_{t+1}+\mathbf{W}_t^{-1}\mathbf{G}_t\bbeta_{t-1})&\ t=2,\ldots,n-1\\
\mathbf{B}_n(\sigma^{-2}_n\mathbf{F}_ny_n+\mathbf{W}_n^{-1}\mathbf{G}_n\bbeta_{n-1})&\ t=n\\
\end{cases}
\end{equation*}
y
\begin{equation*}
\mathbf{B}_t=
\begin{cases}
(\sigma^{-2}_1\mathbf{F}_1\mathbf{F}_1'+\mathbf{G}_2'\mathbf{W}_2^{-1}\mathbf{G}_2+\mathbf{R}_1^{-1})^{-1}&\ t=1\\
(\sigma^{-2}_t\mathbf{F}_t\mathbf{F}_t'+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\mathbf{G}_{t+1}+\mathbf{W}_t^{-1})^{-1}&\ t=2,\ldots,n-1\\
(\sigma^{-2}_n\mathbf{F}_n\mathbf{F}_n'+\mathbf{W}_n^{-1})^{-1}&\ t=n\\
\end{cases}
\end{equation*}
\end{Res}
\begin{proof}
\begin{align*}
&\ \ \ \ p(\bbeta_t \mid y_1,\ldots,y_n,\bbeta_1,\ldots,\bbeta_{t-1},\bbeta_{t+1}, \ldots,\bbeta_n)\\&\propto p(y_1,\ldots,y_n,\bbeta_1,\ldots,\bbeta_n)\\
&=p(y_1 \mid \bbeta_1,\ldots,\bbeta_n,y_2,\ldots,y_n)p(y_2 \mid \bbeta_1,\ldots,\bbeta_n,y_3,\ldots,y_n)\ldots p(y_n \mid \bbeta_1,\ldots,\bbeta_n)p(\bbeta_1,\ldots,\bbeta_n)\\
&=p(y_1 \mid \bbeta_1)\ldots p(y_n \mid \bbeta_n)p(\bbeta_1,\ldots,\bbeta_n)\\
&\propto p(y_t \mid \bbeta_t)p(\bbeta_1,\ldots,\bbeta_n)\\
&=p(y_t \mid \bbeta_t)p(\bbeta_n \mid \bbeta_{n-1},\ldots,\bbeta_1)p(\bbeta_{n-1} \mid \bbeta_{n-2},\ldots,\bbeta_1)\ldots p(\bbeta_1)\\
&= p(y_t \mid \bbeta_t)p(\bbeta_n \mid \bbeta_{n-1})p(\bbeta_{n-1} \mid \bbeta_{n-2})\ldots p(\bbeta_1)\\
&\propto p(y_t \mid \bbeta_t)p(\bbeta_{t+1} \mid \bbeta_{t-1})p(\bbeta_t \mid \bbeta_{t-1})
\end{align*}

para $t=2,\cdots,n-2$. Mientras que para $t=1$
\begin{equation}\label{posbeta1}
p(\bbeta_1 \mid y_1,\ldots,y_n,\bbeta_2, \ldots,\bbeta_n)\propto p(y_1 \mid \bbeta_1)p(\bbeta_2 \mid \bbeta_1)p(\bbeta_1),
\end{equation}
y para $t=n$
\begin{equation}\label{posbetan}
p(\bbeta_n \mid y_1,\ldots,y_n,\bbeta_1, \ldots,\bbeta_{n-1})\propto p(y_n \mid \bbeta_n)p(\bbeta_n \mid \bbeta_{n-1}).
\end{equation}
En primer lugar, consideramos $t=2,\ldots,n-2$. Recurriendo a la ecuación de observación y la distribución normal de $\epsilon_t$, se tiene que $y_t \mid \bbeta_t\sim N(\mathbf{F}_t'\bbeta_t,\sigma_t^2)$; por otro lado, la ecuación de sistema y la distribución normal de $\boldsymbol{\omega}_t$ conducen a $\bbeta_{t+1} \mid \bbeta_t\sim N_p(\mathbf{G}_{t+1}\bbeta_t,\mathbf{W}_{t+1})$ y $\bbeta_t \mid \bbeta_{t-1}\sim N_p(\mathbf{G}_t\bbeta_{t-1},\mathbf{W}_t)$. Usando estas distribuciones, se tiene que
\begin{align*}
&\ \ \ \ \ \ p(\bbeta_t \mid y_1,\ldots,y_n)\\&\propto
\exp\left\{-\frac{1}{2\sigma_t^2}(y_t-\mathbf{F}_t'\bbeta_t)^2\right\}\exp\left\{-\frac{1}{2}(\bbeta_{t+1}-\mathbf{G}_{t+1}\bbeta_t)'\mathbf{W}_{t+1}^{-1}(\bbeta_{t+1}-\mathbf{G}_{t+1}\bbeta_t)\right\}\\
&\hspace{6cm}\exp\left\{-\frac{1}{2}(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})'\mathbf{W}_t^{-1}(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})\right\}\\
&\propto \exp\left\{\frac{1}{2}\left[\bbeta_t'(\sigma^{-2}_t\mathbf{F}_t\mathbf{F}_t'+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\mathbf{G}_{t+1}+\mathbf{W}_t^{-1})\bbeta_t-2\bbeta_t'(\sigma^{-2}_t\mathbf{F}_ty_t+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\bbeta_{t+1}+\mathbf{W}_t^{-1}\mathbf{G}_t\bbeta_{t-1})\right]\right\}
\end{align*}
Al definir $\mathbf{B}_t=(\sigma^{-2}_t\mathbf{F}_t\mathbf{F}_t'+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\mathbf{G}_{t+1}+\mathbf{W}_t^{-1})^{-1}$, se tiene que
\begin{align*}
p(\bbeta_t \mid y_1,\ldots,y_n)&\propto\exp\left\{\frac{1}{2}\left[\bbeta_t'\mathbf{B}_t^{-1}\bbeta_t-2\bbeta_t'\mathbf{B}_t^{-1}\mathbf{B}_t(\sigma^{-2}_t\mathbf{F}_ty_t+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\bbeta_{t+1}+\mathbf{W}_t^{-1}\mathbf{G}_t\bbeta_{t-1})\right]\right\}\\
&=\exp\left\{\frac{1}{2}\left[\bbeta_t'\mathbf{B}_t^{-1}\bbeta_t-2\bbeta_t'\mathbf{B}_t^{-1}\mathbf{b}_t\right]\right\}
\end{align*}
con $\mathbf{b}_t=\mathbf{B}_t(\sigma^{-2}_t\mathbf{F}_ty_t+\mathbf{G}_{t+1}'\mathbf{W}_{t+1}^{-1}\bbeta_{t+1}+\mathbf{W}_t^{-1}\mathbf{G}_t\bbeta_{t-1})$, de donde se concluye que $\bbeta_t \mid y_1,\ldots,y_n\sim N_p(\mathbf{b}_t,\mathbf{B}_t)$ para $t=2,\ldots,n-2$.

Ahora, considera $t=1$, en cuyo caso la distribución a posteriori de $\bbeta_1$ está dada en (\ref{posbeta1}). Tenemos que $y_1 \mid \bbeta_1\sim N(\mathbf{F}_1'\bbeta_1,\sigma_1^2)$, $\bbeta_2 \mid \bbeta_1\sim N_p(\mathbf{G}_2\bbeta_1,\mathbf{W}_2)$ y $\bbeta_1\sim N_p(\mathbf{a}_1,\mathbf{R}_1)$. Entonces tenemos que
\begin{align*}
&\ \ \ \ \ \ p(\bbeta_1 \mid y_1,\ldots,y_n)\\
&\propto p(y_1 \mid \bbeta_1)p(\bbeta_2 \mid \bbeta_1)p(\bbeta_1)\\
&\propto \exp\left\{-\frac{1}{2\sigma_1^2}(y_1-\mathbf{F}_1'\bbeta_1)^2\right\}\exp\left\{-\frac{1}{2}(\bbeta_2-\mathbf{G}_2\bbeta_1)'\mathbf{W}_2^{-1}(\bbeta_2-\mathbf{G}_2\bbeta_1)\right\}\\
&\hspace{6cm}\exp\left\{-\frac{1}{2}(\bbeta_1-\mathbf{a}_1)'\mathbf{R}_1^{-1}(\bbeta_1-\mathbf{a}_1)\right\}\\
&\propto \exp\left\{\frac{1}{2}\left[\bbeta_1'(\sigma^{-2}_1\mathbf{F}_1\mathbf{F}_1'+\mathbf{G}_2'\mathbf{W}_2^{-1}\mathbf{G}_2+\mathbf{R}_1^{-1})\bbeta_1-2\bbeta_1'(\sigma^{-2}_1\mathbf{F}_1y_1+\mathbf{G}_2'\mathbf{W}_2^{-1}\bbeta_2+\mathbf{R}_1^{-1}\mathbf{a}_1)\right]\right\}\\
&=\exp\left\{\frac{1}{2}\left[\bbeta_1'\mathbf{B}_1^{-1}\bbeta_1-2\bbeta_1'\mathbf{B}_1^{-1}\mathbf{B}_1(\sigma^{-2}_1\mathbf{F}_1y_1+\mathbf{G}_2'\mathbf{W}_2^{-1}\bbeta_2+\mathbf{R}_1^{-1}\mathbf{a}_1)\right]\right\}\\
&=\exp\left\{\frac{1}{2}\left[\bbeta_1'\mathbf{B}_1^{-1}\bbeta_1-2\bbeta_1'\mathbf{B}_1^{-1}\mathbf{b}_1\right]\right\}\\
\end{align*}
con $\mathbf{B}_1=(\sigma^{-2}_1\mathbf{F}_1\mathbf{F}_1'+\mathbf{G}_2'\mathbf{W}_2^{-1}\mathbf{G}_2+\mathbf{R}_1^{-1})^{-1}$ y $\mathbf{b}_1=\mathbf{B}_1(\sigma^{-2}_1\mathbf{F}_1y_1+\mathbf{G}_2'\mathbf{W}_2^{-1}\bbeta_2+\mathbf{R}_1^{-1}\mathbf{a}_1)$. De donde se concluye que $\bbeta_1 \mid y_1,\ldots,y_n\sim N_p(\mathbf{b}_1,\mathbf{B}_1)$.

El procedimiento para $t=n$ es análogo, y queda demostrado el resultado.
\end{proof}
Las distribuciones del anterior resultado permite llevar a cabo un algoritmo de muestreador de Gibbs. (Ver Gaberman MCMC, p. 173)

Para encontrar la distribución suavizada $\bbeta_t \mid y^n$ para $t=1,\ldots,n$, se encuentra en primer lugar la distribución suavizada conjunta de $\bbeta_1$, $\ldots$, $\bbeta_2$. Tenemos que
\begin{align}\label{betapos}
p(\bbeta_1,\ldots,\bbeta_n \mid y^n)&=\frac{p(\bbeta_1,\ldots,\bbeta_n,y^n)}{p(y^n)}\notag\\
&=\frac{p(\bbeta_1 \mid \bbeta_2,\ldots,\bbeta_n \mid y^n)p(\bbeta_2 \mid \bbeta_3,\ldots,\bbeta_n,y^n)\ldots p(\bbeta_{n-1} \mid \bbeta_n,y^n)p(\bbeta_n,y^n)}{p(y^n)}\notag\\
&=p(\bbeta_1 \mid \bbeta_2,\ldots,\bbeta_n \mid y^n)p(\bbeta_2 \mid \bbeta_3,\ldots,\bbeta_n,y^n)\ldots p(\bbeta_{n-1} \mid \bbeta_n,y^n)p(\bbeta_n \mid y^n)\notag\\
&=p(\bbeta_n \mid y^n)\prod_{i=1}^{n-1}p(\bbeta_i \mid \bbeta_{i+1},\ldots,\bbeta_n,y^n)\notag\\
&=p(\bbeta_n \mid y^n)\prod_{i=1}^{n-1}p(\bbeta_i \mid \bbeta_{i+1},y^i)
\end{align}
la última igualdad se tiene por el hecho de que dado $\bbeta_{i+1}$, $\bbeta_i$ es independiente de $y_j$ y $\bbeta_j$ con $j>i$.

Integrando con respecto a $\bbeta_1$ la ecuación (\ref{betapos}), se tiene que
\begin{align*}
p(\bbeta_2,\ldots,\bbeta_n \mid y^n)&=p(\bbeta_n \mid y^n)\prod_{i=2}^{n-1}p(\bbeta_i \mid \bbeta_{i+1},y^t)\int p(\bbeta_1 \mid \bbeta_2,\ldots,\bbeta_n,y^t)d\bbeta_1\\
&=p(\bbeta_n \mid y^n)\prod_{i=2}^{n-1}p(\bbeta_i \mid \bbeta_{i+1},y^i),
\end{align*}
puesto que la integral de una función de densidad condicional vale 1. Análogamente integrando con respecto a $\bbeta_2$, $\ldots$, $\bbeta_{t-1}$, se tiene que
\begin{equation*}
p(\bbeta_t,\ldots,\bbeta_n \mid y^n)=p(\bbeta_n \mid y^n)\prod_{i=t}^{n-1}p(\bbeta_i \mid \bbeta_{i+1},y^i),
\end{equation*}
y además
\begin{align*}
p(\bbeta_t,\bbeta_{t+1} \mid y^n)&=\frac{p(\bbeta_t,\bbeta_{t+1},y^n)}{p(y^n)}\\
&=\frac{p(\bbeta_t \mid \bbeta_{t+1},y^n)p(\bbeta_{t+1},y^n)}{p(y^n)}\\
&=p(\bbeta_t \mid \bbeta_{t+1},y^n)p(\bbeta_{t+1} \mid y^n)\\
&=p(\bbeta_t \mid \bbeta_{t+1},y^t)p(\bbeta_{t+1} \mid y^n)
\end{align*}


\begin{Res}
\begin{equation}
\bbeta_t \mid y^n\sim N(\mathbf{m}^n_t,\mathbf{P}^n_t)
\end{equation}
con $\mathbf{m}^n_t=\mathbf{m}_t+\mathbf{P}_t\mathbf{G}'_{t+1}\mathbf{R}_{t+1}^{-1}(\mathbf{m}^n_{t+1}-\mathbf{a}_{t+1})$ y $\mathbf{P}^n_t=\mathbf{P}_t-\mathbf{P}_t\mathbf{G}'_{t+1}\mathbf{R}^{-1}_{t+1}(\mathbf{R}_{t+1}-\mathbf{P}^n_{t+1})\mathbf{R}^{-1}_{t+1}\mathbf{G}_{t+1}\mathbf{P}_t$
\end{Res}
En síntesis, el algoritmo del suavizamiento consta de los siguientes pasos:
\begin{enumerate}
\item Obtener las distribuciones filtrados $\bbeta_1 \mid y^1$, $\bbeta_2 \mid y ^2$, $\ldots$, $\bbeta_n \mid y^n$.
\item Obtener las distribuciones suavizados $\bbeta_1 \mid y^n$, $\bbeta_2 \mid y^n$, $\ldots$, $\bbeta_n \mid y^n$. Esto se lleva a cabo de la siguiente forma,
    \begin{enumerate}[(a)]
    \item usando la distribución de $\bbeta_n \mid y^n\sim N(\mathbf{m}_n,\mathbf{P}_n)$ y notando que $\mathbf{m}_n=\mathbf{m}_n^n$ y $\mathbf{P}_n=\mathbf{P}_n^n$, calcular $\mathbf{m}_{n-1}^n$ y $\mathbf{P}_{n-1}^n$, y así obtener la distribución de $\bbeta_{n-1} \mid y^n$.
    \item usar la distribución de $\bbeta_{n-1} \mid y^n$ para encontrar la de $\bbeta_{n-2} \mid y^n$ similarmente.
    \item Repetir el proceso hasta obtener la distribución de $\bbeta_1 \mid y^n$.
    \end{enumerate}
\end{enumerate}

\subsection{$\sigma^2_t$ y $\mathbf{W}_t$ desconocidos}
Cuando $\sigma^2_t$ y $\mathbf{W}_t$, las varianzas de los errores $\epsilon_t$ y $\boldsymbol{\omega}_t$ son desconocidas, las distribuciones encontradas anteriormente siguen siendo válidas condicionando sobre $\sigma^2_t$ y $\mathbf{W}_t$.

Para encontrar las distribuciones concernientes a $\sigma^2_t$ y $\mathbf{W}_t$, suponga que éstas son constantes a través del tiempo, esto es, $\sigma^2_t=\sigma^2$ y $\mathbf{W}_t=\mathbf{W}$ para $t=1,\ldots,n$. Tenemos las siguientes distribuciones a posteriori de $\sigma^2$ y $\mathbf{W}$.
\begin{align*}
&\ \ \ \ \ p(\sigma^2 \mid \bbeta,\mathbf{W},y_1,\ldots,y_n)\\
&=\frac{p(\sigma^2,\bbeta,\mathbf{W},y_1,\ldots,y_n)}{p(\bbeta,\mathbf{W},y_1,\ldots,y_n)}\\
&\propto p(y_1 \mid \sigma^2,\bbeta,\mathbf{W},y_2,\ldots,y_n)p(y_2 \mid \sigma^2,\bbeta,\mathbf{W},y_3,\ldots,y_n)\ldots p(y_n \mid \sigma^2,\bbeta,\mathbf{W})p(\sigma^2,\bbeta,\mathbf{W})\\
&=p(y_1 \mid \sigma^2,\bbeta_1)p(y_2 \mid \sigma^2,\bbeta_2)\ldots p(y_n \mid \sigma^2,\bbeta_n)p(\sigma^2 \mid \bbeta,\mathbf{W})p(\bbeta,\mathbf{W})\\
&\propto \prod_{i=1}^np(y_i \mid \sigma^2,\bbeta_i)p(\sigma^2 \mid \bbeta,\mathbf{W})
\end{align*}
y de forma análogo, se tiene que
\begin{equation*}
p(\mathbf{W} \mid \bbeta,\sigma^2,y_1,\ldots,y_n)\propto\prod_{i=2}^np(\bbeta_i \mid \bbeta_{i-1},\mathbf{W})p(\mathbf{W} \mid \bbeta,\sigma^2)
\end{equation*}
Suponiendo que las distribuciones a priori son $\sigma^2\sim IG(n_\sigma/2,n_\sigma S_\sigma/2)$ y $\mathbf{W}\sim IW(n_W/2,n_WS_W/2)$, y además son independientes, entonces
\begin{align*}
&\ \ \ \ \ p(\sigma^2 \mid \bbeta,\mathbf{W},y_1,\ldots,y_n)\\
&\propto \prod_{i=1}^n(2\pi\sigma^2)^{-1/2}\exp\left\{-\frac{1}{2\sigma^2}(y_i-\mathbf{F}'_i\bbeta_i)^2\right\}\frac{(n_\sigma S_\sigma/2)^{n_\sigma/2}}{\Gamma(n_\sigma/2)}(\sigma^{-2})^{\frac{n_\sigma}{2}-1}\exp\left\{-\frac{n_\sigma S_\sigma}{2\sigma^2}\right\}\\
&\propto (\sigma^{-2})^{\frac{n+n_\sigma}{2}-1}\exp\left\{-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n(y_i-\mathbf{F}'_i\bbeta_i)^2+n_\sigma S_\sigma\right]\right\}\\
&=(\sigma^{-2})^{\frac{n_\sigma^*}{2}-1}\exp\left\{-\frac{1}{2\sigma^2}n_\sigma^*S_\sigma^*\right\}
\end{align*}
con $n_\sigma^*=n+n_\sigma$ y $n_\sigma^* S_\sigma^*=\sum_{i=1}^n(y_i-\mathbf{F}'_i\bbeta_i)^2+n_\sigma S_\sigma$. De lo anterior se concluye que $\sigma^2 \mid \bbeta,\mathbf{W},y_1,\ldots,y_n\sim IG(n_\sigma^*/2,n_\sigma^* S_\sigma^*/2)$.

Por otro lado,
\begin{align*}
&\ \ \ \ p(\mathbf{W} \mid \bbeta,\sigma^2,y_1,\ldots,y_n)\\
&\propto \prod_{i=2}^n \mid \mathbf{W} \mid ^{1/2}\exp\left\{-\frac{1}{2}(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})'\mathbf{W}^{-1}(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})\right\} \mid \mathbf{W} \mid ^{-\frac{n_W}{2}+\frac{p+1}{2}}\exp\left\{-tr(\frac{n_WS_W}{2}\mathbf{W}^{-1})\right\}\\
&\propto \mid \mathbf{W} \mid ^{-\frac{n-1}{2}-\frac{n_W}{2}+\frac{p+1}{2}}\exp\left\{-tr(\frac{1}{2}\sum_{i=2}^n (\bbeta_t-\mathbf{G}_t\bbeta_{t-1})(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})'\mathbf{W}^{-1})\right\}\exp\left\{-tr(\frac{n_WS_W}{2}\mathbf{W}^{-1})\right\}\\
&= \mid \mathbf{W} \mid ^{-\frac{n+n_W-1}{2}+\frac{p+1}{2}}\exp\left\{-tr\left[\frac{1}{2}(\sum_{i=2}^n (\bbeta_t-\mathbf{G}_t\bbeta_{t-1})(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})'+n_WS_W)\mathbf{W}^{-1}\right]\right\}\\
&= \mid \mathbf{W} \mid ^{-\frac{n_W^*}{2}+\frac{p+1}{2}}\exp\left\{-tr\left[\frac{1}{2}n_W^*S_W^*\mathbf{W}^{-1}\right]\right\}\\
\end{align*}
con $n_W^*=n_W+n-1$ y $n_W^*S_W^*=\sum_{i=2}^n (\bbeta_t-\mathbf{G}_t\bbeta_{t-1})(\bbeta_t-\mathbf{G}_t\bbeta_{t-1})'+n_WS_W$. De lo anterior se concluye que $\mathbf{W} \mid \bbeta,\sigma^2,y_1,\ldots,y_n\sim IW(n_W^*/2,n_W^*S_W^*/2)$.

\section{Ejercicios}
\begin{enumerate}
\item Encuentra una representación en modelo de espacio-estado para los modelo $AR(p)$ diferente a la dada en el ejemplo \ref{AR_p} definiendo el vector de estado como $\bbeta=(y_t,\phi_2y_{t-1},\phi_3y_{t-2},\cdots,\phi_py_{t-p+1})'$.
\item En el ejemplo \ref{ModEstBas}, escribe la ecuación de estado asumiendo que el componente estacional $\gamma_t$ es trigonométrico definido como sigue:
\begin{align*}
\gamma_t=\sum_{j=1}^{[s/2]}\gamma_{jt}
\end{align*}

donde
\begin{align*}
\gamma_{jt}&=\gamma_{j,t-1}\cos\lambda_j+\gamma*_{j,t-1}\sin\lambda_j+\omega_{jt}\\
\gamma*_{jt}&=-\gamma_{j,t-1}\sin\lambda_j+\gamma*_{j,t-1}\cos\lambda_j+\omega*_{jt}
\end{align*}

$\{\omega_{jt}\}$ y $\{\omega*_{jt}\}$ procesos ruido blanco incorrelacionados con varianza común $\sigma^2_j$ para $j=1,\cdots,[s/2]$.
\item Demostrar la ecuación (\ref{demostrarP}).
\end{enumerate}
