\chapter{Modelos en poblaciones finitas}

\section{Diseños estadísticos}

\citeasnoun{Gelman03} afirma que se debe ser un estadístico ingenuo si se afirma que toda inferencia debería ser condicional a los datos, sin importar de dónde o cómo fueron seleccionados. Esta es una concepción errada del principio de verosimilitud. La noción de que el método de selección de la muestra es irrelevante en el análisis inferencial puede ser contradicha con un argumento muy simple: Suponga que se tienen a disposición diez datos provenientes del lanzamiento de diez dados; todos ellos correspondieron al número seis. La actitud del estadístico acerca de la naturaleza de los datos sería diferente si (1) sólo se hicieron diez lanzamientos, (2) se hicieron sesenta lanzamientos pero se decidió reportar sólo los que resultaron ser seis, (3) aparecieron diez seis en quinientos lanzamientos y se decidió reportar ´honestamente estas realizaciones. En tales situaciones es claro que la distribución de los datos observados sigue un patrón completamente distinto que no debe ser ignorado.

Una \textbf{población finita} es un conjunto de $N$ elementos $\{e_1, e_2, ..., e_N\}$. Cada unidad puede ser identificada sin ambigüedad por un conjunto de rótulos. Sea $U=\{1,2,...,N\}$ el conjunto de rótulos de la población finita\footnote{El tamaño de la población no es necesariamente conocido.}. En términos generales, es el conjunto de unidades que conforman el universo de estudio. $N$ es comúnmente llamado el tamaño poblacional. Se utilizará el subíndice $i$ para denotar la existencia física del $i$-ésimo elemento. En algunas ocasiones el objetivo de la investigación es poder estimar el tamaño de la población.

Para resolver los objetivos del estudio se recurre al planteamiento de una estrategia de muestreo con el fin de seleccionar una \textbf{muestra aleatoria} que representa un subconjunto de la población que ha sido extraído mediante un mecanismo estadístico de selección. Notaremos con una letra mayúscula $S$ a la muestra aleatoria\footnote{Nótese que $S$ es una variable aleatoria.} y con una letra minúscula $s$ a una realización de la misma. De tal forma que, sin ambigüedad, una muestra seleccionada (realizada) es el conjunto de unidades pertenecientes a

\begin{equation*}
s=\{1,...,k,...,n\}.
\end{equation*}

El número de componentes de $s$ es llamado el \textbf{tamaño de muestra} y no siempre es fijo. Es decir, en algunos casos $n$ es una cantidad aleatoria. El conjunto de todas las posibles muestras se conoce como \textbf{soporte}. Haciendo una analogía con la inferencia estadística clásica, el soporte generado por una muestra aleatoria corresponde al espacio muestral generado por una variable aleatoria.

En términos generales, un diseño de muestreo no es sino una distribución de probabilidad multivariante definida sobre un conjunto de muestras que pertenecen a un soporte. Pero, una distribución de probabilidad no es más sino un modelo que se asume; en este caso, es un modelo que permite la selección de muestras probabilísticas. Una muestra $s$ induce un vector de inclusión $\mathbf{I}(s)=(I_1(s),\ldots,I_k(s),\ldots,I_N(s))'$, en donde $I_k(s)$ está definida por (2.1.8). Dado el anterior esquema, otra forma de denotar el diseño de muestreo es $p(\mathbf{I})$ el cual se conoce para todos los posibles valores de $\mathbf{I}$ en todas las posibles muestras $s$. Por otro lado, si se asume que la medición de la característica de interés $y_k$ en los individuos de la población está sujeta a un error, entonces éstas deben ser vistas como realizaciones de variables aleatorias $Y_k$. De esta forma, es necesario definir un modelo para los valores poblacionales que puede depender de cierto parámetro. En este caso, si $Y=(Y_1,\ldots,Y_k,\ldots,Y_N)'$ es el vector poblacional de la característica de interés, entonces $p(\mathbf{Y} \mid \theta)$ definirá tal modelo.

Para realizar cualquier tipo de inferencias acerca del parámetro $\theta$ es necesario trabajar con una distribución de probabilidad conjunta de ($\mathbf{I},\mathbf{Y}$) que permita unificar todo el esquema anterior en un sólo proceso. La pregunta que atañe al estadístico es la siguiente: ¿cómo se puede expresar esa distribución conjunta en términos de $p(\mathbf{I})$ y de $p(\mathbf{I},\theta)$? \citeasnoun{CharSki} dan la respuesta a esta pregunta motivando la suposición de que $\mathbf{Y}$ sea independiente de $\mathbf{I}$. En algunos caso como en \citeasnoun[capítulo 8]{CharSki} el diseño de muestreo depende de los valores de la característica de interés; por ejemplo, en un estudio en de casos y controles, la respuesta $y_k$ es de tipo binario, indicando si la $k$-ésima unidad corresponde a un caso o a un control. A su vez, los casos y controles inducen estratos cuyas muestras son seleccionadas independientemente. En este caso, el diseño de muestreo depende directamente de los valores de la característica de interés. Por lo tanto, la relación entre $\mathbf{I},\mathbf{Y}$ debe ser expresada como
\begin{equation*}
p(\mathbf{I},\mathbf{Y} \mid \theta)=
p(\mathbf{I} \mid \mathbf{Y})p(\mathbf{Y} \mid \theta)
\end{equation*}

En este caso, se dice que el diseño de muestreo es \emph{informativo} y no puede ser ignorado en términos de inferencia para $\theta$. Por otro lado, si el diseño de muestreo es \emph{no informativo}, la relación entre $\mathbf{I},\mathbf{Y}$ debe ser expresada como
\begin{equation*}
p(\mathbf{I},\mathbf{Y} \mid \theta)=
p(\mathbf{I})p(\mathbf{Y} \mid \theta)
\end{equation*}

y claramente, el diseño de muestreo puede ser ignorado. \citeasnoun{CharSki} afirman que los diseños de muestreo que dependen directamente de la variable de interés no son raros en la práctica. Sin emabargo, los diseños de muestreo implementados cuando el marco de muestreo es muy deficiente como el muestreo en dos fases, en donde se selecciona una primera muestra y con base en los resultados de esta se diseña la estrategia para una segunda submuestra, no puede ser catalogado como no informativo y por tanto no puede ser ignorado. Por otro lado, es más común encontrar que el diseño de muestreo dependa de otras variables de información auxiliar, como en el diseño estratificado o el diseño proporcional al tamaño. A continuación se presenta el marco general dado por \citeasnoun{Valliant2000} para modelar conjuntamente el diseño de muestreo y el mecanismo probabilístico que origina a la variable de interés. Además, \citeasnoun{Sudgen1984} afirman que diseños de muestreo como el aleatorio simple, aleatorio estratificado, proporcional al tamaño, el muestreo a conveniencia o el muestreo balanceado corresponde a casos en donde es posible ignorar el mecanismo de selección. También concluyen que aunque algunas veces los diseños de muestreo pueden ser ignorados en términos de inferencia para $\bbeta$, es equivocado pensar que siempre pueden ser ignorados en términos de inferencia predictiva para el total poblacional $T_y$.

Aunque en este capítulo sólo abordaremos la inferencia Bayesiana en donde el diseño de muestreo se considera no informativo, pero los autores recalcan que en la práctica los diseños estadísticos son mayormente complejos y no pueden ser ignorados tan fácilmente. Por ejemplo, el siguiente diseño de muestreo, conocido como diseño aleatorio simple, es muy utilizado en la práctica en las últimas etapas de diseños de muestreo complejos.
\begin{equation}
p(\mathbf{I})=
\begin{cases}
\frac{1}{\binom{N}{n}} &\text{si $\sum_{k\in U}I_k=n$}\\
0  &\text{en otro caso}
      \end{cases}
\end{equation}

Claramente este diseño de muestreo no depende de la característica de interés y puede catalogarse como no informativo. Otro diseño de muestreo con estas características es el diseño aleatorio estratificado dado por
\begin{equation}
p(\mathbf{I})=
\begin{cases}
\prod_{h=1}^H\frac{1}{\binom{N_h}{n_h}}, &\text{si $\sum_{h=1}^Hn_h=n$}\\
0,                                       &\text{en otro caso}
\end{cases}
\end{equation}

Los anteriores diseños de muestreo cumplen la siguiente propiedad
\begin{equation*}
p(\mathbf{I},\mathbf{Y} \mid \theta)=
p(\mathbf{I})p(\mathbf{Y} \mid \theta)
\end{equation*}

y por lo tanto en términos de inferencia pueden ser ignorados. Este capítulo está enfocado en la inferencia de parámetros para poblaciones finitas cuando el diseño de muestreo es ignorado. La inferencia bayesiana para poblaciones finitas supone el uso de información auxiliar que relaciona a la característica de interés con las variables auxiliares mediante un modelo de superpoblación, $\xi$. Bajo esta perspectiva la observación de la característica de interés en las unidades poblacionales $y_k$ se define como la realización de una variable aleatoria $Y_k$. Partiendo de que el total poblacional se puede escribir como

\begin{equation}
T_y=\sum_{k\in s} Y_k+\sum_{k\notin s} Y_k,
\end{equation}

la tarea es estimar por medio del modelo bayesiano, las respectivas observaciones $y_k$ de los elementos que no fueron seleccionados en la muestra. Denotando esta estimación como $E(Y_k)$, un predictor para el total estaría dado por:

\begin{equation}
\hat{T}_y=\sum_{k\in s} Y_k+\sum_{k\notin s} E_{\xi}(Y_k)
\end{equation}

y por tanto la realización de $\hat{T}_y$ con los datos específicos de la muestra seleccionada $s$ estaría definida como

\begin{equation}
\hat{t}_y=\sum_{k\in s} y_k + \sum_{k\notin s} \hat{E}_{\xi}(Y_k)
\end{equation}

donde $\hat{E}_{\xi}(Y_k)$ es una estimación de $E_{\xi}(Y_k)$ realizada con los datos obtenidos de la muestra seleccionada $s$. Suponga que el vector de las variables de interés es $\mathbf{Y}=(Y_1,Y_2,\ldots,Y_N)'$ y para cada elemento de la población la realización de estas variables aleatorias es $\mathbf{y}=(y_1,y_2,\ldots,y_N)'$. Suponga que el objetivo es estimar una combinación lineal\footnote{Si el objetivo es estimar el total poblacional, entonces $\mathbf{l}'=(1,1,\ldots,1)$. Si el objetivo es estimar la media poblacional, entonces $\mathbf{l}'=(1/N,1/N,\ldots,1/N)$.} $\mathbf{l}'\mathbf{y}$. Para tal fin, se selecciona una muestra $s$ de tamaño $n$. Nótese que tanto $\mathbf{y}$ como $\mathbf{l}$ se pueden particionar de la siguiente manera: $\mathbf{y}=(\mathbf{y}_s',\mathbf{y}_r')'$ y $\mathbf{l}=(\mathbf{l}_s', \mathbf{l}_r')'$; en donde el subíndice $s$ se refiere a que el vector contiene los $n$ elementos de la muestra seleccionada y el subíndice $r$ se refiere a que el vector contiene los $N-n$ elementos que no fueron seleccionados en la muestra. En este orden de ideas, \citeasnoun{Bol} desarrollaron el siguiente marco de referencia con los siguientes resultados que dan cuenta de la estimación de parámetros poblacionales de interés en inferencia de poblaciones finitas como lo son totales o medias.

\begin{Res}
Para cualquier cantidad lineal poblacional $\mathbf{l}'\mathbf{Y}$, el predictor Bayesiano que minimiza la pérdida del error cuadrático bajo cualquier modelo, para el cual $Var_{\xi}(\mathbf{Y}_r \mid \mathbf{Y}_s)$ existe, está dado por
\begin{equation}
\mathbf{l}_s'\mathbf{Y}_s+\mathbf{l}'_rE_{\xi}(\mathbf{Y}_r \mid \mathbf{Y}_s)
\end{equation}
\end{Res}

Existe una infinidad de modelos bayesianos que se pueden proponer, por tanto la lógica que sigue este capítulo es la revisión de los modelos más simples que van a ser generalizados en secciones posteriores para dar así una visión amplia, más no exhaustiva, del espectro que siguen los modelos bayesianos en inferencia de poblaciones finitas.

\section{Modelo simple}

Suponga que $\mathbf{Y}=(Y_1,\ldots,Y_N)'$ y además asuma que el modelo de superpoblación $\xi$ que rige el comportamiento de la estructura probabilística de la variable de interés en la población finita está dado por
\begin{equation}
Y_i=\theta+e_i \ \ \ \ \ \ \ \ \ \ i=1,\ldots,N
\end{equation}

Además suponga que $e_i\sim N(0,\sigma^2)$ y que las distribuciones \emph{a priori} de cada una de las variables de interés y del parámetro $\theta$ son
\begin{align*}
Y_i \mid \theta &\sim N(\theta,\sigma^2)\\
\theta &\sim N(\mu,\tau^2)
\end{align*}

Donde $\sigma^2$, $\mu$ y $\tau^2$ se asumen conocidas. Asuma entonces que cada uno de los errores del modelo $e_i$ es independiente de $\theta$ para todo $i=1,\ldots,N$.

\begin{Res}
Si $\mathbf{Y}$ se particiona como $\mathbf{Y}=(\mathbf{Y}_r',\mathbf{Y}_s')$, donde $\mathbf{Y}_r$ y $\mathbf{Y}_s$ son de tamaño $N-n$ y $n$, respectivamente. Entonces se tiene que $\mathbf{Y}_r \mid \mathbf{Y}_s$ tiene distribución normal multivariante con esperanza común $\mu_r=\dfrac{n\bar{y}_s\tau^2+\mu\sigma^2}{n\tau^2+\sigma^2}$, varianza común $\tau_r=\sigma^2+\dfrac{\sigma^2\tau^2}{n\tau^2+\sigma^2}$ y covarianza común $\dfrac{\sigma^2\tau^2}{n\tau^2+\sigma^2}$.
\end{Res}
\begin{proof}
En primer lugar, tenemos las siguientes propiedades acerca de las variables $Y_1$, $\ldots$, $Y_N$. Para $i=1,\ldots,N$, tenemos que
\begin{equation*}
E(Y_i)=E(\theta+e_i)=\mu,
\end{equation*}

\begin{equation*}
Var(Y_i)=Var(\theta)+Var(e_i)=\tau^2+\sigma^2,
\end{equation*}
y
\begin{align*}
Cov(Y_i,Y_j)&=Cov(\theta+e_i,\theta+e_j)\\
&=Var(\theta)+Cov(e_i,e_j)\\
&=\tau^2
\end{align*}
para $i\neq j$.

Ahora consideramos el estimador de $\theta$ en la muestra observada $\bar{Y}_s$. Usando $Y_i \mid \theta\sim N(\theta,\sigma^2)$ para $i=1,\ldots,n$, se tiene que $\bar{Y}_s \mid \theta\sim N(\theta,\sigma^2/n)$, además $\theta\sim N(\mu,\tau^2)$, entonces utilizando el resultado A.3.5., se tiene que el vector $(\bar{Y}_s,\theta)$ tiene distribución normal bivariante. Para encontrar el vector de medias y matriz de varianzas y covarianzas, tenemos
\begin{equation*}
E(\bar{Y}_s)=E(Y_1)=\mu,
\end{equation*}

\begin{align*}
Var(\bar{Y}_s)&=\frac{1}{n^2}Var(\sum_{k\in s}Y_k)\\
&=\frac{1}{n^2}\left[\sum_{k\in s}Var(Y_k)+\sum\sum_{i\neq j}Cov(Y_i,Y_j)\right]\\
&=\frac{1}{n^2}\left[n(\tau^2+\sigma^2)+(n^2-n)\tau^2\right]\\
&=\tau^2+\frac{\sigma^2}{n}
\end{align*}
y
\begin{align*}
Cov(\bar{Y}_s,\theta)&=\frac{1}{n}\sum_{k\in s}Cov(Y_k,\theta)\\
&=\frac{1}{n}\sum_{k\in s}\tau^2\\
&=\tau^2.
\end{align*}
En conclusión, se tiene que
\begin{equation*}
\begin{pmatrix}
\bar{Y}_s\\
\theta
\end{pmatrix}\sim N_2\left(\begin{pmatrix}
\mu\\
\mu
\end{pmatrix},\begin{pmatrix}
\tau^2+\frac{\sigma^2}{n}&\tau^2\\
\tau^2&\tau^2
\end{pmatrix}\right).
\end{equation*}
Ahora, usando la anterior distribución y la propiedad 4 del resultado A.3.3 de la distribución multivariante, se tiene que
\begin{equation*}
\theta \mid \bar{Y}_s\sim N(\mu+\tau^2(\tau^2+\frac{\sigma^2}{n})^{-1}(\bar{y}_s-\mu),\tau^2-\tau^2(\tau^2+\frac{\sigma^2}{n})^{-1}\tau^2).
\end{equation*}
Para simplificar la esperanza y la varianza de esta distribución, tenemos
\begin{align*}
\mu+\tau^2(\tau^2+\frac{\sigma^2}{n})^{-1}(\bar{y}_s-\mu)&=\mu+\frac{n\tau^2}{n\tau^2+\sigma^2}(\bar{y}_s-\mu)\\
&=\frac{n\tau^2\bar{y}_s}{n\tau^2+\sigma^2}+(1-\frac{n\tau^2}{n\tau^2+\sigma^2})\mu\\
&=\frac{n\tau^2\bar{y}_s+\sigma^2\mu}{n\tau^2+\sigma^2},
\end{align*}
y
\begin{align*}
\tau^2-\tau^2(\tau^2+\frac{\sigma^2}{n})^{-1}\tau^2&=\tau^2-\frac{n\tau^4}{n\tau^2+\sigma^2}\\
&=\tau^2(1-\frac{n\tau^2}{n\tau^2+\sigma^2})\\
&=\frac{\tau^2\sigma^2}{n\tau^2+\sigma^2}.
\end{align*}
En conclusión,
\begin{equation*}
\theta \mid \bar{Y}_s\sim N(\frac{n\tau^2\bar{y}_s+\sigma^2\mu}{n\tau^2+\sigma^2},\frac{\tau^2\sigma^2}{n\tau^2+\sigma^2}).
\end{equation*}
Ahora, por hipótesis, se tiene que
\begin{equation*}
\begin{pmatrix}
\mathbf{Y}_r\\
\mathbf{Y}_s
\end{pmatrix} \mid \theta\sim N\left(\begin{pmatrix}
\theta\mathbf{1}_{N-n}\\
\theta\mathbf{1}_{n}
\end{pmatrix},\begin{pmatrix}
\sigma^2\mathbf{I}_{N-n}&\mathbf{0}\\
\mathbf{0}&\sigma^2\mathbf{I}_n
\end{pmatrix}\right),
\end{equation*}
de donde
\begin{equation*}
\mathbf{Y}_r \mid \mathbf{Y}_s,\theta\sim N(\theta\mathbf{1}_{N-n},\sigma^2\mathbf{I}_{N-n}).
\end{equation*}
Por otro lado, la estadística $\mathbf{Y}_s$ es suficiente bayesianamente, \cite{Zack} entonces la distribución de interés $\mathbf{Y}_r \mid \mathbf{Y}_s$ es la misma de $\mathbf{Y}_r \mid \bar{Y}_s$. Y podemos encontrar la distribución de $\mathbf{Y}_r \mid \mathbf{Y}_s$ usando la distribución de $(\mathbf{Y}_r',\mathbf{Y}_s)'$ encontrada anteriormente. Tenemos que
\begin{align}\label{Y_rY_s}
p(\mathbf{Y}_r \mid \bar{Y}_s)&=\frac{p(\mathbf{Y}_r,\bar{Y}_s)}{p(\bar{Y}_s)}\notag\\
&=\frac{\int p(\mathbf{Y}_r \mid \bar{Y}_s,\theta)p(\theta \mid \bar{Y}_s)p(\bar{Y}_s)d\theta}{p(\bar{Y}_s)}\notag\\
&=\int p(\mathbf{Y}_r \mid \bar{Y}_s,\theta)p(\theta \mid \bar{Y}_s)d\theta.
\end{align}
De nuevo, usando la suficiencia de $\bar{Y}_s$, se tiene que la distribución de $\mathbf{Y}_r \mid \bar{Y}_s,\theta$ coincide con la de $\mathbf{Y}_r \mid \mathbf{Y}_s,\theta$ que corresponde a una distribución normal multivariante. Entonces la expresión $p(\mathbf{Y}_r \mid \bar{Y}_s,\theta)$ contiene una forma cuadrática de $\mathbf{Y}_r$, y por consiguiente, también lo contiene la expresión (\ref{Y_rY_s}), de donde se concluye que la distribución de $\mathbf{Y}_r \mid \bar{Y}_s$ es normal multivariante. Para encontrar la respectiva esperanza y varianza. Tenemos que
\begin{align*}
E(\mathbf{Y}_r \mid \bar{Y}_s)&=E(E(\mathbf{Y}_r \mid \mathbf{Y}_s,\theta) \mid \bar{Y}_s)\\
&=E(\theta\mathbf{1}_{N-n} \mid \bar{Y}_s)\\
&=\mathbf{1}_{N-n}E(\theta \mid \bar{Y}_s)\\
&=\mathbf{1}_{N-n}\frac{n\tau^2\bar{y}_s+\sigma^2\mu}{n\tau^2+\sigma^2}.
\end{align*}
Y
\begin{align*}
Var(\mathbf{Y}_r \mid \bar{Y}_s)&=E(Var(\mathbf{Y}_r \mid \mathbf{Y}_s,\theta) \mid \bar{Y}_s)+Var(E(\mathbf{Y}_r \mid \mathbf{Y}_s,\theta) \mid \bar{Y}_s)\\
&=E(\sigma^2\mathbf{I}_{N-n} \mid \bar{Y}_s)+Var(\theta\mathbf{I}_{N-n} \mid \bar{Y}_s)\\
&=\sigma^2\mathbf{I}_{N-n}+\mathbf{1}_{(N-n)\times(N-n)}\frac{\tau^2\sigma^2}{n\tau^2+\sigma^2},
\end{align*}
donde $\mathbf{1}_{(N-n)\times(N-n)}$ denota la matriz de dimensión $(N-n)\times(N-n)$ de entradas iguales a 1. En conclusión, $\mathbf{Y}_r \mid \mathbf{Y}_s$ tiene distribución normal multivariante con esperanza común $\dfrac{n\bar{y}_s\tau^2+\mu\sigma^2}{n\tau^2+\sigma^2}$, varianza común $\sigma^2+\dfrac{\sigma^2\tau^2}{n\tau^2+\sigma^2}$ y covarianza común $\dfrac{\sigma^2\tau^2}{n\tau^2+\sigma^2}$.
\end{proof}

Con base en este resultado, se procede a la utilización de XXXXXX para poder calcular las predicciones para los parámetros de interés en la encuesta o el estudio por muestreo.

\begin{Res}
Para cualquier cantidad lineal poblacional $\mathbf{l}'\mathbf{Y}$, el predictor Bayesiano que minimiza la pérdida del error cuadrático bajo el modelo simple está dado por
\begin{equation}
\mathbf{l}_s'\mathbf{Y}_s+\mathbf{l}'_r\bmu_r
\end{equation}
donde $\bmu_r=(\mu_r,\ldots,\mu_r)'$
\end{Res}

En particular la predicción para el total poblacional $T_y$ está dada por la siguiente expresión
\begin{equation}
\hat{T}_y=\sum_{k\in s}Y_k + (N-n)\mu_r
\end{equation}

Si el parámetro de interés es la media poblacional, entonces el predictor Bayesiano estaría dado por
\begin{equation}
\bar{Y}_U=\frac{\hat{T}_y}{N}=\frac{\sum_{k\in s}Y_k + (N-n)\mu_n}{N}
\end{equation}

\section{Modelo general}

Asuma que el modelo de superpoblación $\xi$ que rige el comportamiento de la estructura probabilística de la variable de interés en la población finita está dado por
\begin{equation}
\mathbf{Y}=\mathbf{X}\bbeta+\mathbf{e}
\end{equation}

Además suponga que las distribuciones \emph{a priori} de cada una del vector de variables de interés y del vector de parámetros $\bbeta$ son
\begin{align*}
\mathbf{Y} \mid \bbeta,\mathbf{X} &\sim N(\mathbf{X}\bbeta,\mathbf{V})\\
\bbeta &\sim N(\mathbf{b},\mathbf{B})
\end{align*}

Donde $\mathbf{V}$ es la matriz de varianzas poblacional y se supone conocida. Además $\mathbf{e}$ y $\bbeta$ son independientes.

\begin{Res}
Si $\mathbf{Y}$ se particiona como $\mathbf{Y}=(\mathbf{Y}_r',\mathbf{Y}_s')$, donde $\mathbf{Y}_r$ y $\mathbf{Y}_s$ son de tamaño $N-n$ y $n$, respectivamente. Entonces se tiene que $\mathbf{Y}_r \mid \mathbf{Y}_s$ tiene distribución normal multivariante con esperanza dada por
\begin{equation*}
E(\mathbf{Y}_r \mid \mathbf{Y}_s)=\bmu_r=\mathbf{X}_r\hat{\bbeta}_B+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s)\hat{\bbeta}_B
\end{equation*}
y matriz de varianzas dada por
\begin{multline*}
Var(\mathbf{Y}_r \mid \mathbf{Y}_s)=\bSigma_r=\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr}+\\(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)(\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_s+\mathbf{B}^{-1})^{-1}(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)',
\end{multline*}
donde $\mathbf{X}_s$, $\mathbf{X}_r$, $\mathbf{V}_r$, $\mathbf{V}_{rs}$ y $\mathbf{V}_{sr}$ corresponden a las submatrices de $\mathbf{X}$ y $\mathbf{V}$ correspondiente a la partición de $\mathbf{Y}$. Y
\begin{equation*}
\hat{\bbeta}_B=(\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_s+\mathbf{B}^{-1})^{-1}(\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{y}_s+\mathbf{B}^{-1}\mathbf{b}).
\end{equation*}
\end{Res}
\begin{proof}
En primer lugar consideramos el estimador de $\bbeta$ que es suficiente en el sentido Bayesiano \cite{Zack} dado por
\begin{equation*}
\hat{\bbeta}_s=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{Y}_s.
\end{equation*}
Por hipótesis, se tiene que $\mathbf{Y}_s \mid \bbeta$ tiene distribución normal multivariante, entonces $\hat{\bbeta}_s \mid \bbeta$ también tiene distribución normal multivariante con
\begin{align*}
E(\hat{\bbeta}_s \mid \bbeta)&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}E(\mathbf{Y}_s \mid \bbeta)\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_s\bbeta\\
&=\bbeta
\end{align*}
y
\begin{align*}
Var(\hat{\bbeta}_s \mid \bbeta)&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}Var(\mathbf{Y}_s \mid \bbeta)\mathbf{V}_s^{-1}\mathbf{X}_s(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{V}_s\mathbf{V}_s^{-1}\mathbf{X}_s(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}.
\end{align*}
En conclusión, $\hat{\bbeta}_s \mid \bbeta\sim N(\bbeta,(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1})$. Por otro lado, $\bbeta\sim N(\mathbf{b},\mathbf{B})$, entonces por el resultado A.3.5, se tiene que $(\hat{\bbeta}_s',\bbeta')$ tiene distribución normal multivariante, con
\begin{align*}
E(\hat{\bbeta}_s)&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}E(\mathbf{Y}_s)\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_sE(\bbeta)\\
&=E(\bbeta)\\
&=\mathbf{b},
\end{align*}

\begin{align*}
Var(\hat{\bbeta}_s)&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}Var(\mathbf{Y}_s)\mathbf{V}_s^{-1}\mathbf{X}_s(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}(\mathbf{X}_s\mathbf{B}\mathbf{X}_s'+\mathbf{V}_s)\mathbf{V}_s^{-1}\mathbf{X}_s(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}+\mathbf{B}
\end{align*}
y
\begin{align*}
Cov(\hat{\bbeta}_s,\bbeta)&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}Cov(\mathbf{Y}_s,\bbeta)\\
&=(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_sCov(\bbeta,\bbeta)\\
&=\mathbf{B}.
\end{align*}
En conclusión, se tiene que
\begin{equation*}
\begin{pmatrix}
\hat{\bbeta}_s\\
\bbeta
\end{pmatrix}\sim N\left(\begin{pmatrix}
\mathbf{b}\\
\mathbf{b}
\end{pmatrix},\begin{pmatrix}
(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}+\mathbf{B}&\mathbf{B}\\
\mathbf{B}&\mathbf{B}
\end{pmatrix}\right).
\end{equation*}
Usando la propiedad 4 del resultado A.3.3, se tiene que $\bbeta \mid \hat{\bbeta}_s$ tiene distribución normal multivariante con esperanza
\begin{equation*}
E(\bbeta \mid \hat{\bbeta}_s)=\mathbf{b}+\mathbf{B}[(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}+\mathbf{B}]^{-1}(\hat{\bbeta}_s-\mathbf{b})
\end{equation*}
al definir $\hat{\bbeta}_B=E(\bbeta \mid \hat{\bbeta}_s)$, se puede ver que $\hat{\bbeta}_B=(\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{X}_s+\mathbf{B}^{-1})^{-1}(\mathbf{X}_s'\mathbf{V}_s^{-1}\mathbf{y}_s+\mathbf{B}^{-1}\mathbf{b})$
y matriz de covarianzas
\begin{equation*}
Var(\bbeta \mid \hat{\bbeta}_s)=\mathbf{B}-\mathbf{B}[(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s)^{-1}+\mathbf{B}]^{-1}\mathbf{B}
\end{equation*}
que resulta ser igual a $(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s+\mathbf{B}^{-1})^{-1}$.

Ahora, por hipótesis se tiene que
\begin{equation*}
\begin{pmatrix}
\mathbf{Y}_r\\
\mathbf{Y}_s
\end{pmatrix} \mid \bbeta,\mathbf{X}\sim N\left(\begin{pmatrix}
\mathbf{X}_r\bbeta\\
\mathbf{X}_s\bbeta
\end{pmatrix},\begin{pmatrix}
\mathbf{V}_r&\mathbf{V}_{rs}\\
\mathbf{V}_{sr}&\mathbf{V}_s
\end{pmatrix}\right),
\end{equation*}
entonces la distribución de $\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta$ es una distribución normal multivariante con
\begin{equation*}
E(\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta)=\mathbf{X}_r\bbeta+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s\bbeta)
\end{equation*}
y
\begin{equation*}
Var(\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta)=\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr}.
\end{equation*}
Ahora, $\hat{\bbeta}_s$ es simplemente una matriz de constantes multiplicado por $\mathbf{Y}_s$, por lo tanto, la distribución de $\mathbf{Y}_r \mid \hat{\bbeta}_s,\bbeta$ también es normal multivariante.

Por otro lado, la suficiencia de $\hat{\bbeta}_s$ implica que la distribución de interés $\mathbf{Y}_r \mid \mathbf{Y}_s$ es la misma que $\mathbf{Y}_r \mid \hat{\bbeta}_s$. Tenemos que
\begin{align}
p(\mathbf{Y}_r \mid \hat{\bbeta}_s)&=\frac{p(\mathbf{Y}_r,\hat{\bbeta}_s)}{p(\hat{\bbeta}_s)}\notag\\
&=\frac{\int p(\mathbf{Y}_r \mid \hat{\bbeta}_s,\bbeta)p(\bbeta \mid \hat{\bbeta}_s)p(\hat{\bbeta}_s)d\bbeta}{p(\hat{\bbeta}_s)}\notag\\
&=\int p(\mathbf{Y}_r \mid \hat{\bbeta}_s,\bbeta)p(\bbeta \mid \hat{\bbeta}_s)d\bbeta.
\end{align}
Como se comentó antes, $\mathbf{Y}_r \mid \hat{\bbeta}_s,\bbeta$ tiene distribución normal multivariante, y por consiguiente, $p(\mathbf{Y}_r \mid \hat{\bbeta}_s,\bbeta)$ contiene una forma cuadrática de $\mathbf{Y}_r$. Luego, $p(\mathbf{Y}_r \mid \hat{\bbeta}_s)$ también contiene esa forma cuadrática, y  se concluye que la distribución de $\mathbf{Y}_r \mid \hat{\bbeta}_s$ es normal multivariante. Entonces se tiene que $\mathbf{Y}_r \mid \mathbf{Y}_s$ también tiene distribución normal multivariante.

Ahora,
\begin{align*}
E(\mathbf{Y}_r \mid \mathbf{Y}_s)&=E(\mathbf{X}_r\bbeta+\mathbf{V}_r \mid \mathbf{Y}_s)\\
&=E(\mathbf{X}_r\bbeta \mid \mathbf{Y}_s)+E(\mathbf{V}_r \mid \mathbf{Y}_s)\\
&=\mathbf{X}_rE(\bbeta \mid \mathbf{Y}_s)+\mathbf{V}_r\\
&=\mathbf{X}_rE(\bbeta \mid \hat{\bbeta}_s)+E(\mathbf{V}_r \mid \hat{\bbeta}_s)\\
&=E(\mathbf{X}_r\bbeta+\mathbf{V}_r \mid \hat{\bbeta}_s)\\
&=E(\mathbf{Y}_r \mid \hat{\bbeta}_s)
\end{align*}

pero
\begin{align*}
E(\mathbf{Y}_r \mid \hat{\bbeta}_s)&=E(E(\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta) \mid \hat{\bbeta}_s)\\
&=E(\mathbf{X}_r\bbeta+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s\bbeta) \mid \hat{\bbeta}_s)\\
&=\mathbf{X}_rE(\bbeta \mid \hat{\bbeta}_s)+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_sE(\bbeta \mid \hat{\bbeta}_s))\\
&=\mathbf{X}_r\hat{\bbeta}_B+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s\hat{\bbeta}_B).
\end{align*}
De donde se concluye que $E(\mathbf{Y}_r \mid \mathbf{Y}_s)=\mathbf{X}_r\hat{\bbeta}_B+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s\hat{\bbeta}_B)$.

Análogamente, se tiene que
\begin{align*}
Var(\mathbf{Y}_r \mid \mathbf{Y}_s)&=Var(\mathbf{Y}_r \mid \hat{\bbeta}_s)\\
&=E(Var(\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta) \mid \hat{\bbeta}_s)+Var(E(\mathbf{Y}_r \mid \mathbf{Y}_s,\bbeta) \mid \hat{\bbeta}_s)\\
&=E(\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr} \mid \hat{\bbeta}_s)+Var(\mathbf{X}_r\bbeta+\mathbf{V}_{rs}\mathbf{V}_s^{-1}(\mathbf{y}_s-\mathbf{X}_s\bbeta) \mid \hat{\bbeta}_s)\\
&=\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr}+Var(\mathbf{X}_r\bbeta-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s\bbeta \mid \hat{\bbeta}_s)\\
&=\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr}+(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)Var(\bbeta \mid \hat{\bbeta}_s)(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)'\\
&=\mathbf{V}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{V}_{sr}+(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)(\mathbf{X}_s'\mathbf{V}_s\mathbf{X}_s+\mathbf{B}^{-1})^{-1}(\mathbf{X}_r-\mathbf{V}_{rs}\mathbf{V}_s^{-1}\mathbf{X}_s)'.
\end{align*}
Con lo anterior, se completa la demostración.
\end{proof}

Con base en el anterior resultado, se procede a la utilización de XXXXXX para poder calcular las predicciones para los parámetros de interés en la encuesta o el estudio por muestreo.

\begin{Res}
Para cualquier cantidad lineal poblacional $\mathbf{l}'\mathbf{Y}$, el predictor Bayesiano que minimiza la pérdida del error cuadrático bajo el modelo simple está dado por
\begin{equation}
\mathbf{l}_s'\mathbf{Y}_s+\mathbf{l}'_r\bmu_r
\end{equation}
\end{Res}

En particular la predicción para el total poblacional $T_y$ está dada por la siguiente expresión
\begin{equation}
\hat{T}_y=\sum_{k\in s}Y_k + \mathbf{1}'_r\mu_r
\end{equation}

donde $\mathbf{1}'_r$ es un vector de unos de tamaño $N-n$. 