<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.1 Modelo Bernoulli | Modelos Bayesianos con R y STAN" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN." />
<meta name="github-repo" content="psirusteam/bookdownBayesiano" />

<meta name="author" content="Andrés Gutiérrez - Hanwen Zhang" />

<meta name="date" content="2021-06-04" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN.">

<title>3.1 Modelo Bernoulli | Modelos Bayesianos con R y STAN</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#prefacio">Prefacio</a></li>
<li><a href="antes-de-comenzar.html#antes-de-comenzar">Antes de comenzar</a>
<ul>
<li><a href="cuestionamientos-sobre-el-enfoque-bayesiano.html#cuestionamientos-sobre-el-enfoque-bayesiano">Cuestionamientos sobre el enfoque bayesiano</a></li>
<li><a href="acerca-de-la-notación.html#acerca-de-la-notación">Acerca de la notación</a></li>
</ul></li>
<li><a href="1-tópicos-básicos.html#tópicos-básicos"><span class="toc-section-number">1</span> Tópicos básicos</a>
<ul>
<li><a href="1-1-teoría-de-la-decisión.html#teoría-de-la-decisión"><span class="toc-section-number">1.1</span> Teoría de la decisión</a></li>
<li><a href="1-2-algunos-resultados-de-probabilidad.html#algunos-resultados-de-probabilidad"><span class="toc-section-number">1.2</span> Algunos resultados de probabilidad</a></li>
<li><a href="1-3-teorema-de-bayes.html#teorema-de-bayes"><span class="toc-section-number">1.3</span> Teorema de Bayes</a></li>
</ul></li>
<li><a href="2-inferencia-bayesiana.html#inferencia-bayesiana"><span class="toc-section-number">2</span> Inferencia bayesiana</a>
<ul>
<li><a href="2-1-la-distribución-previa.html#la-distribución-previa"><span class="toc-section-number">2.1</span> La distribución previa</a>
<ul>
<li><a href="2-1-la-distribución-previa.html#distribuciones-conjugadas"><span class="toc-section-number">2.1.1</span> Distribuciones conjugadas</a></li>
<li><a href="2-1-la-distribución-previa.html#familia-exponencial"><span class="toc-section-number">2.1.2</span> Familia exponencial</a></li>
<li><a href="2-1-la-distribución-previa.html#distribuciones-previas-no-informativas"><span class="toc-section-number">2.1.3</span> Distribuciones previas no informativas</a></li>
</ul></li>
<li><a href="2-2-pruebas-de-hipótesis.html#pruebas-de-hipótesis"><span class="toc-section-number">2.2</span> Pruebas de hipótesis</a>
<ul>
<li><a href="2-2-pruebas-de-hipótesis.html#factor-de-bayes"><span class="toc-section-number">2.2.1</span> Factor de Bayes</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#valor-p-bayesiano"><span class="toc-section-number">2.2.2</span> Valor-<span class="math inline">\(p\)</span> Bayesiano</a></li>
</ul></li>
<li><a href="2-3-criterios-de-información.html#criterios-de-información"><span class="toc-section-number">2.3</span> Criterios de información</a>
<ul>
<li><a href="2-3-criterios-de-información.html#criterio-dic"><span class="toc-section-number">2.3.1</span> Criterio DIC</a></li>
<li><a href="2-3-criterios-de-información.html#criterios-aic-y-bic"><span class="toc-section-number">2.3.2</span> Criterios AIC y BIC</a></li>
</ul></li>
</ul></li>
<li><a href="3-modelos-uniparamétricos.html#modelos-uniparamétricos"><span class="toc-section-number">3</span> Modelos uniparamétricos</a>
<ul>
<li><a href="3-1-modelo-bernoulli.html#modelo-bernoulli"><span class="toc-section-number">3.1</span> Modelo Bernoulli</a></li>
<li><a href="3-2-modelo-binomial.html#modelo-binomial"><span class="toc-section-number">3.2</span> Modelo Binomial</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li><a href="A-elementos-de-probabilidad.html#elementos-de-probabilidad"><span class="toc-section-number">A</span> Elementos de probabilidad</a>
<ul>
<li><a href="A-1-distribuciones-discretas.html#distribuciones-discretas"><span class="toc-section-number">A.1</span> Distribuciones discretas</a>
<ul>
<li><a href="A-1-distribuciones-discretas.html#distribución-uniforme-discreta"><span class="toc-section-number">A.1.1</span> Distribución uniforme discreta</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-hipergeométrica"><span class="toc-section-number">A.1.2</span> Distribución hipergeométrica</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-bernoulli"><span class="toc-section-number">A.1.3</span> Distribución Bernoulli</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-binomial"><span class="toc-section-number">A.1.4</span> Distribución binomial</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-binomial-negativa"><span class="toc-section-number">A.1.5</span> Distribución Binomial negativa</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-de-poisson"><span class="toc-section-number">A.1.6</span> Distribución de Poisson</a></li>
</ul></li>
<li><a href="A-2-distribuciones-continuas.html#distribuciones-continuas"><span class="toc-section-number">A.2</span> Distribuciones continuas</a>
<ul>
<li><a href="A-2-distribuciones-continuas.html#distribución-uniforme-continua"><span class="toc-section-number">A.2.1</span> Distribución Uniforme Continua</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-weibull"><span class="toc-section-number">A.2.2</span> Distribución Weibull</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-valor-extremo"><span class="toc-section-number">A.2.3</span> Distribución valor-extremo</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-gamma"><span class="toc-section-number">A.2.4</span> Distribución Gamma</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-gamma-inversa"><span class="toc-section-number">A.2.5</span> Distribución Gamma-inversa</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-exponencial"><span class="toc-section-number">A.2.6</span> Distribución exponencial</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-beta"><span class="toc-section-number">A.2.7</span> Distribución Beta</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-normal"><span class="toc-section-number">A.2.8</span> Distribución normal</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-log-normal"><span class="toc-section-number">A.2.9</span> Distribución log-normal</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-ji-cuadrado"><span class="toc-section-number">A.2.10</span> Distribución Ji-cuadrado</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-t-student"><span class="toc-section-number">A.2.11</span> Distribución t-student</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-t-student-generalizada"><span class="toc-section-number">A.2.12</span> Distribución t-student generalizada</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-f"><span class="toc-section-number">A.2.13</span> Distribución F</a></li>
</ul></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribuciones-multivariadas"><span class="toc-section-number">A.3</span> Distribuciones multivariadas</a>
<ul>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-multinomial"><span class="toc-section-number">A.3.1</span> Distribución Multinomial</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-dirichelt"><span class="toc-section-number">A.3.2</span> Distribución Dirichelt</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-normal-multivariante"><span class="toc-section-number">A.3.3</span> Distribución Normal Multivariante</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-wishart"><span class="toc-section-number">A.3.4</span> Distribución Wishart</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-inversa-wishart"><span class="toc-section-number">A.3.5</span> Distribución inversa-Wishart</a></li>
</ul></li>
</ul></li>
<li><a href="B-matriz-de-información.html#matriz-de-información"><span class="toc-section-number">B</span> Matriz de información</a></li>
<li><a href="C-elementos-de-simulación-estadística.html#elementos-de-simulación-estadística"><span class="toc-section-number">C</span> Elementos de simulación estadística</a>
<ul>
<li><a href="C-1-métodos-directos.html#métodos-directos"><span class="toc-section-number">C.1</span> Métodos directos</a>
<ul>
<li><a href="C-1-métodos-directos.html#método-de-la-transformación-uniforme"><span class="toc-section-number">C.1.1</span> Método de la transformación uniforme</a></li>
<li><a href="C-1-métodos-directos.html#método-de-la-grilla"><span class="toc-section-number">C.1.2</span> Método de la grilla</a></li>
</ul></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#métodos-de-monte-carlo-vía-cadenas-de-markov"><span class="toc-section-number">C.2</span> Métodos de Monte Carlo vía cadenas de Markov</a>
<ul>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#el-muestreador-de-gibbs"><span class="toc-section-number">C.2.1</span> El muestreador de Gibbs</a></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#el-algoritmo-de-metrópolis-hastings"><span class="toc-section-number">C.2.2</span> El algoritmo de Metrópolis-Hastings</a></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#buenas-prácticas-en-la-aplicación-de-métodos-mcmc"><span class="toc-section-number">C.2.3</span> Buenas prácticas en la aplicación de métodos MCMC</a></li>
</ul></li>
</ul></li>
<li><a href="referencias.html#referencias">Referencias</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="modelo-bernoulli" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Modelo Bernoulli</h2>
<p>Suponga que <span class="math inline">\(Y\)</span> es una variable aleatoria con distribución Bernoulli
dada por:</p>
<p><span class="math display">\[\begin{equation}
p(Y \mid \theta)=\theta^y(1-\theta)^{1-y}I_{\{0,1\}}(y)
\end{equation}\]</span></p>
<p>Como el parámetro <span class="math inline">\(\theta\)</span> está restringido al espacio <span class="math inline">\(\Theta=[0,1]\)</span>,
entonces es posible formular varias opciones para la distribución previa
del parámetro. En particular, la distribución uniforme restringida al
intervalo <span class="math inline">\([0,1]\)</span> o la distribución Beta parecen ser buenas opciones.
Puesto que la distribución uniforme es un caso particular de la
distribución Beta, entonces se iniciará con ésta. Por lo tanto la
distribución previa del parámetro <span class="math inline">\(\theta\)</span> estará dada por</p>
<p><span class="math display" id="eq:betadistribution">\[\begin{equation}
\tag{3.1}
p(\theta \mid \alpha,\beta)=
\frac{1}{Beta(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}I_{[0,1]}(\theta).
\end{equation}\]</span></p>
<p>Bajo este marco de referencia se tienen los siguientes resultados</p>

<div class="proposition">
<span id="prp:unnamed-chunk-1" class="proposition"><strong>Resultado 3.1  </strong></span>La distribución posterior del parámetro <span class="math inline">\(\theta\)</span> sigue una distribución
<span class="math display">\[\begin{equation*}
\theta \mid Y \sim Beta(y+\alpha,\beta-y+1)
\end{equation*}\]</span>
</div>

<div class="proof">
<p> <span class="proof"><em>Prueba. </em></span> <span class="math display">\[\begin{align*}
p(\theta \mid Y)&amp;\propto 
p(Y \mid \theta)p(\theta \mid \alpha,\beta)\\
&amp;=\frac{I_{\{0,1\}}(y)}{Beta(\alpha,\beta)}\theta^y\theta^{\alpha-1}(1-\theta)^{\beta-1}(1-\theta)^{1-y}I_{[0,1]}(\theta)\\
&amp;\propto \theta^{y+\alpha-1}(1-\theta)^{\beta-y+1-1}I_{[0,1]}(\theta)
\end{align*}\]</span></p>
Por lo tanto, factorizando convenientemente, se encuentra una expresión idéntica a la función de distribución de una variable aleatoria con distribución <span class="math inline">\(Beta(y+\alpha,\beta-y+1)\)</span>.
</div>
<p><br></p>
<p>Del anterior resultado, podemos ver que la familia de distribuciones
Beta es conjugada con respecto a la familia de distribuciones Bernoulli.
Ahora consideremos cuál sería la distribución previa no informativa de
Jeffreys para el parámetro <span class="math inline">\(\theta\)</span>. De acuerdo a la definición
<a href="2-1-la-distribución-previa.html#def:Jeffreys">2.2</a>, se tiene que</p>
<p><span class="math display">\[\begin{equation*}
p(\theta)\propto I(\theta) ^{1/2}
\end{equation*}\]</span></p>
<p>En donde <span class="math inline">\(I(\theta)\)</span> es la información de Fisher del parámetro <span class="math inline">\(\theta\)</span>,
que en este caso está dada por</p>
<p><span class="math display">\[\begin{align*}
I(\theta)&amp;=
-E\left\{\dfrac{\partial^2}{\partial\theta^2}\log{p(\mathbf{Y}\mid\theta)}\right\}\\
&amp;=-E\left\{\dfrac{\partial^2}{\partial\theta^2}\{Y\log\theta+(1-Y)\log(1-\theta)\}\right\}\\
&amp;=E\left\{\frac{Y}{\theta^2}+\frac{1-Y}{(1-\theta)^2}\right\}\\
&amp;=\frac{1}{\theta(1-\theta)}
\end{align*}\]</span></p>
<p>De esta forma, la distribución previa no informativa de Jeffreys debe
ser proporcional a <span class="math inline">\(\theta^{-1/2}(1-\theta)^{-1/2}\)</span>, que asimismo
corresponde a la distribución <span class="math inline">\(Beta(1/2,1/2)\)</span>, cuya función de densidad
se muestra en la figura <a href="3-1-modelo-bernoulli.html#fig:jefber1">3.1</a> la cual asigna iguales pesos
a los valores extremos del parámetro de interés y su característica de
ser no informativa se representa en la simetría de la función alrededor
del valor 0.5.</p>
<div class="figure"><span id="fig:jefber1"></span>
<img src="3Uniparametricos_files/figure-html/jefber1-1.png" alt="Distribución previa no informativa de Jeffreys para el parámetro de una distribución Bernoulli" width="672" />
<p class="caption">
Figura 3.1: Distribución previa no informativa de Jeffreys para el parámetro de una distribución Bernoulli
</p>
</div>

<div class="proposition">
<span id="prp:unnamed-chunk-3" class="proposition"><strong>Resultado 2.2  </strong></span>La distribución predictiva previa para una observación <span class="math inline">\(y\)</span> está dada por
<span class="math display" id="eq:Predipreviabernou">\[\begin{equation}
\tag{3.2}
p(Y)=
\frac{Beta(y+\alpha,\beta-y+1)}{Beta(\alpha,\beta)}I_{\{0,1\}}(y)
\end{equation}\]</span>
La cual define una auténtica función de densidad de probabilidad continua.
</div>
<p><br></p>

<div class="proof">
<p> <span class="proof"><em>Prueba. </em></span> De la definición de función de distribución predictiva se tiene que</p>
<p><span class="math display">\[\begin{align*}
p(Y)&amp;=\int p(Y \mid \theta)p(\theta \mid \alpha,\beta)\ d\theta\\
&amp;=\int_0^1 \theta^y(1-\theta)^{1-y}I_{\{0,1\}}(y)\frac{1}{Beta(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}\ d\theta\\
&amp;=\frac{Beta(y+\alpha,\beta-y+1)}{Beta(\alpha,\beta)}I_{\{0,1\}}(y)
    \int_0^1\frac{\theta^{y+\alpha-1}(1-\theta)^{\beta-y+1-1}}{Beta(y+\alpha,\beta-y+1)}\ d\theta\\
&amp;=\frac{Beta(y+\alpha,\beta-y+1)}{Beta(\alpha,\beta)}I_{\{0,1\}}(y)
\end{align*}\]</span></p>
<p>Nótese que en la anterior expresión, la integral al lado derecho de la tercera igualdad es igual a la unidad, puesto que la expresión matemática dentro de la integral corresponde a la función de densidad de una variable aleatoria con distribucion <span class="math inline">\(Beta\)</span>, que tiene rango en el intervalo <span class="math inline">\((0,1)\)</span>. Por otro lado se deben verificar las dos condiciones de función de densidad. Es decir</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p(Y)&gt;0 \ \forall y \in Y)\)</span>. Esta condición se tiene trivialmente puesto que la función matemática Beta siempre toma valores positivos.</li>
<li><span class="math inline">\(\int p(y)\ dx=1\)</span>. En este caso, esta función es discreta definida en el conjunto <span class="math inline">\(\{0,1\}\)</span>. Por lo tanto esta condición es equivalente a
<span class="math display">\[\begin{equation*}
\sum_{y\in{\{0,1\}}}P(Y=y)=
  \sum_{y\in{\{0,1\}}}\frac{Beta(y+\alpha,\beta-y+1)}{Beta(\alpha,\beta)}
=1
\end{equation*}\]</span>
Lo cual se verifica fácilmente teniendo en cuenta las propiedades de la función matemática Beta y de la función matemática Gamma.
</div>
<br></li>
</ol>
<p>La distribución predictiva dada en <a href="3-1-modelo-bernoulli.html#eq:Predipreviabernou">(3.2)</a> está
basada únicamente en la distribución previa del parámetro <span class="math inline">\(\theta\)</span>. Una
vez observada la variable <span class="math inline">\(Y\)</span> se puede pensar en actualizar la
distribución predictiva basando la inferencia en la distribución
posterior del parámetro; esta distribución se da en el siguiente
resultado.</p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-5" class="proposition"><strong>Resultado 2.3  </strong></span>Después de la recolección de los datos, la distribución predictiva posterior para una nueva observación <span class="math inline">\(\tilde{y}\)</span> está dada por</p>
<span class="math display">\[\begin{equation}
p(\tilde{y} \mid Y)=
  \frac{Beta(\tilde{y}+y+\alpha,\beta-\tilde{y}-y+2)}{Beta(y+\alpha,\beta-y+1)}I_{\{0,1\}}(\tilde{y}),
\end{equation}\]</span>
</div>
<p><br></p>

<div class="proof">
<p> <span class="proof"><em>Prueba. </em></span> De la definición de función de distribución predictiva se tiene que</p>
<span class="math display">\[\begin{align*}
p(\tilde{y} \mid Y)&amp;=\int p(\tilde{y} \mid \theta)p(\theta \mid Y)\ d\theta\\
&amp;=\int_0^1\theta^{\tilde{y}}(1-\theta)^{1-\tilde{y}}I_{\{0,1\}}(\tilde{y})\frac{\theta^{y+\alpha-1}(1-\theta)^{\beta-y+1-1}}{Beta(y+\alpha,\beta-y+1)}\ d\theta\\
&amp;=\frac{Beta(\tilde{y}+y+\alpha,\beta-\tilde{y}-y+2)}{Beta(y+\alpha,\beta-y+1)}I_{\{0,1\}}(\tilde{y})\\
&amp;\hspace{2cm}\times \int_0^1\frac{\theta^{\tilde{y}+y+\alpha-1}(1-\theta)^{\beta-\tilde{y}-y+2-1}}{Beta(\tilde{y}+y+\alpha,\beta-\tilde{y}-y+2)}\ d\theta\\
&amp;=\frac{Beta(\tilde{y}+y+\alpha,\beta-\tilde{y}-y+2)}{Beta(y+\alpha,\beta-y+1)}I_{\{0,1\}}(\tilde{y})
\end{align*}\]</span>
</div>
<p><br></p>
<p>En la práctica rara vez se observa la realización de una única variable
aleatoria Bernoulli <span class="math inline">\(Y\)</span>, sino una muestra de variables aleatorias <span class="math inline">\(Y_1\)</span>,
<span class="math inline">\(\cdots\)</span>, <span class="math inline">\(Y_n\)</span>. En este caso, la distribución posterior del parámetro
<span class="math inline">\(\theta\)</span> está dada en el siguiente resultado.</p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-7" class="proposition"><strong>Resultado 2.4  </strong></span>Cuando se tiene una muestra aleatoria <span class="math inline">\(Y_1,\ldots,Y_n\)</span> de variables con distribución Bernoulli de parámetro <span class="math inline">\(\theta\)</span>, entonces la distribución posterior del parámetro de interés es</p>
<span class="math display">\[\begin{equation*}
\theta \mid Y_1,\ldots,Y_n \sim Beta\left(\sum_{i=1}^ny_i+\alpha,\beta-\sum_{i=1}^ny_i+n\right)
\end{equation*}\]</span>
</div>

<div class="example">
<p><span id="exm:bernoelectoral" class="example"><strong>Ejemplo 3.1  </strong></span>Es común en muchos países del mundo que se presenten encuestas de opinión electoral unas semanas antes de las elecciones presidenciales. Dentro de este tipo de encuestas se acostumbra a indagar acerca del favoritismo de los candidatos involucrados en la contienda electoral. Suponga que el candidato presidencial A está interesado en conocer su intención de voto previa a las elecciones. Para esto, él contrata a una firma encuestadora para la realización de una encuesta entre la población votante. El resultado de este estudio puede hacer cambiar o afirmar las estrategias publicitarias y la redefinición de la campaña electoral. La firma encuestadora decide implementar una estrategia de muestreo con un tamaño de muestra de doce mil personas. A cada respondiente se le realiza la siguiente pregunta:</p>
<blockquote>
<p>Si las elecciones presidenciales fueran mañana. ¿Usted votaría por el candidato A?</p>
</blockquote>
<p>Las respuestas a esta pregunta son realizaciones de una muestra aleatoria de doce mil variables con densidad Bernoulli. Los resultados del estudio arrojan que 6360 personas de las personas entrevistadas, es decir un 53%, votarían por el suscrito candidato. Técnicamente se debe analizar esta cifra puesto que las implicaciones de ganar en una primera vuelta son grandes en el sentido económico, logístico y administrativo. Claramente, el dato 53% asegura una ventaja dentro de la muestra de doce mil personas. Sin embargo, es necesario realizar un estudio más profundo acerca de la caracterización estructural de la intención de voto del candidato en el electorado.</p>
<p>Con base en lo anteriormente expuesto, se decide utilizar la inferencia bayesiana puesto que existe información previa de un estudio anterior, contratado por el mismo candidato unos meses atrás en donde se entrevistaron a mil personas, con un favoritismo que estaba alrededor del 35 por ciento. Esta situación conlleva a la utilización de la metodología bayesiana que incorpora la información pasada acerca del mismo fenómeno.</p>
<p>El estadístico de la firma encuestadora decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como <span class="math inline">\(\alpha\)</span> igual al número de votantes a favor y <span class="math inline">\(\beta\)</span> igual al número de votantes en contra. Es decir, <span class="math inline">\(Beta(\alpha=350, \beta=650)\)</span>. Por lo anterior, la distribución posterior del parámetro de interés, que representa la probabilidad de éxito en las elecciones presidenciales, es <span class="math inline">\(Beta(6360+350, 650-6360+12000)=Beta(6710, 6290)\)</span>. Por lo tanto, utilizando la distribución posterior, se estima que la intención de voto por el candidato es de <span class="math inline">\(\frac{6710}{6710+6290}=\frac{6710}{13000}=0.516\)</span> y este valor equivale a la media de la distribución posterior.</p>
<p>Sin embargo, si no se tuviese información previa como la suministrada por el estudio de meses anteriores, el análisis bayesiano sugeriría trabajar con una distribución previa no informativa, que en este caso, correspondería a una <span class="math inline">\(Beta(\alpha=0.5, \beta=0.5)\)</span>. siguiendo el mismo análisis, se tiene que la distribución posterior es <span class="math inline">\(Beta(6360.5, 5640.5)\)</span>. Finalmente, se estimaría que la intención de voto por el candidato es de <span class="math inline">\(\frac{6350.5}{12001}=0.529\)</span>.</p>
La figuras <a href="3-1-modelo-bernoulli.html#fig:BernoEj1">3.2</a> muestra el comportamiento de las distribuciones previas y posteriores en ambos escenarios. Nótese que la distribución no informativa influye muy poco en el comportamiento de la distribución posterior.
</div>
<div class="figure"><span id="fig:BernoEj1"></span>
<img src="3Uniparametricos_files/figure-html/BernoEj1-1.png" alt="Distribuciones previas (línea punteada) y posteriores (línea sólida) para el ejemplo de las encuestas electorales." width="672" />
<p class="caption">
Figura 3.2: Distribuciones previas (línea punteada) y posteriores (línea sólida) para el ejemplo de las encuestas electorales.
</p>
</div>
<p>Utilizando el siguiente código en R, es posible conocer los intervalos
de credibilidad para las dos distribuciones posteriores. Además, es
posible concluir que, en ambos escenarios, el candidato aventaja
significativamente a sus contrincantes y, salvo algún cambio drástico en
el comportamiento del electorado, ganará las elecciones. Lo anterior se
deduce puesto que el intervalo de credibilidad al 95 % no contiene
ningún valor menor a 0.5</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="3-1-modelo-bernoulli.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">6710</span>, <span class="dv">6290</span>)</span></code></pre></div>
<pre><code>## [1] 0.5075614 0.5247415</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="3-1-modelo-bernoulli.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fl">6350.5</span>, <span class="fl">5640.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.5206678 0.5385340</code></pre>
<p>Por otro lado, el siguiente código en <code>STAN</code> permite obtener el mismo
tipo de inferencia creando cuatro cadenas cuya distribución de
probabilidad coincide con la distribución posterior del ejemplo.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="3-1-modelo-bernoulli.html#cb5-1" aria-hidden="true" tabindex="-1"></a>Bernoulli <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb5-2"><a href="3-1-modelo-bernoulli.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb5-3"><a href="3-1-modelo-bernoulli.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; n;</span></span>
<span id="cb5-4"><a href="3-1-modelo-bernoulli.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="st">  int y[n];</span></span>
<span id="cb5-5"><a href="3-1-modelo-bernoulli.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-6"><a href="3-1-modelo-bernoulli.html#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb5-7"><a href="3-1-modelo-bernoulli.html#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="st">  real&lt;lower=0, upper=1&gt; theta;</span></span>
<span id="cb5-8"><a href="3-1-modelo-bernoulli.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-9"><a href="3-1-modelo-bernoulli.html#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb5-10"><a href="3-1-modelo-bernoulli.html#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="st">  y ~ bernoulli(theta);</span></span>
<span id="cb5-11"><a href="3-1-modelo-bernoulli.html#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="st">  theta ~ beta(350, 650);</span></span>
<span id="cb5-12"><a href="3-1-modelo-bernoulli.html#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb5-13"><a href="3-1-modelo-bernoulli.html#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="3-1-modelo-bernoulli.html#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb5-15"><a href="3-1-modelo-bernoulli.html#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="3-1-modelo-bernoulli.html#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstan)</span>
<span id="cb5-17"><a href="3-1-modelo-bernoulli.html#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">mc.cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb5-18"><a href="3-1-modelo-bernoulli.html#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="3-1-modelo-bernoulli.html#cb5-19" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">12000</span></span>
<span id="cb5-20"><a href="3-1-modelo-bernoulli.html#cb5-20" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">6350</span></span>
<span id="cb5-21"><a href="3-1-modelo-bernoulli.html#cb5-21" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, s), <span class="fu">rep</span>(<span class="dv">0</span>, n <span class="sc">-</span> s))</span>
<span id="cb5-22"><a href="3-1-modelo-bernoulli.html#cb5-22" aria-hidden="true" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">n =</span> n, <span class="at">y =</span> y)</span>
<span id="cb5-23"><a href="3-1-modelo-bernoulli.html#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="3-1-modelo-bernoulli.html#cb5-24" aria-hidden="true" tabindex="-1"></a>Berfit <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">model_code =</span> Bernoulli, </span>
<span id="cb5-25"><a href="3-1-modelo-bernoulli.html#cb5-25" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> sample_data, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>La siguiente salida de <code>STAN</code> permite conocer la estimación bayesiana posterior y los límites del intervalo de credibilidad al 95%.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="3-1-modelo-bernoulli.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(Berfit, <span class="at">pars =</span> <span class="st">&quot;theta&quot;</span>, </span>
<span id="cb6-2"><a href="3-1-modelo-bernoulli.html#cb6-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">digits =</span> <span class="dv">4</span>, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>## Inference for Stan model: 62d7c91114fcc6227deb68f6059b2b09.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean     sd   2.5% 97.5% n_eff   Rhat
## theta 0.5156   1e-04 0.0043 0.5072 0.524  1584 1.0006
## 
## Samples were drawn using NUTS(diag_e) at Fri Jun  4 23:06:22 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
<p style="text-align: center;">
<a href="3-modelos-uniparamétricos.html"><button class="btn btn-default">Previous</button></a>
<a href="3-2-modelo-binomial.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
