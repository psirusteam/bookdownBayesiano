<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="A.3 Distribuciones multivariadas | Modelos Bayesianos con R y STAN" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN." />
<meta name="github-repo" content="psirusteam/bookdownBayesiano" />

<meta name="author" content="Andrés Gutiérrez - Hanwen Zhang" />

<meta name="date" content="2021-06-04" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN.">

<title>A.3 Distribuciones multivariadas | Modelos Bayesianos con R y STAN</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#prefacio">Prefacio</a></li>
<li><a href="antes-de-comenzar.html#antes-de-comenzar">Antes de comenzar</a>
<ul>
<li><a href="cuestionamientos-sobre-el-enfoque-bayesiano.html#cuestionamientos-sobre-el-enfoque-bayesiano">Cuestionamientos sobre el enfoque bayesiano</a></li>
<li><a href="acerca-de-la-notación.html#acerca-de-la-notación">Acerca de la notación</a></li>
</ul></li>
<li><a href="1-tópicos-básicos.html#tópicos-básicos"><span class="toc-section-number">1</span> Tópicos básicos</a>
<ul>
<li><a href="1-1-teoría-de-la-decisión.html#teoría-de-la-decisión"><span class="toc-section-number">1.1</span> Teoría de la decisión</a></li>
<li><a href="1-2-algunos-resultados-de-probabilidad.html#algunos-resultados-de-probabilidad"><span class="toc-section-number">1.2</span> Algunos resultados de probabilidad</a></li>
<li><a href="1-3-teorema-de-bayes.html#teorema-de-bayes"><span class="toc-section-number">1.3</span> Teorema de Bayes</a></li>
</ul></li>
<li><a href="2-inferencia-bayesiana.html#inferencia-bayesiana"><span class="toc-section-number">2</span> Inferencia bayesiana</a>
<ul>
<li><a href="2-1-la-distribución-previa.html#la-distribución-previa"><span class="toc-section-number">2.1</span> La distribución previa</a>
<ul>
<li><a href="2-1-la-distribución-previa.html#distribuciones-conjugadas"><span class="toc-section-number">2.1.1</span> Distribuciones conjugadas</a></li>
<li><a href="2-1-la-distribución-previa.html#familia-exponencial"><span class="toc-section-number">2.1.2</span> Familia exponencial</a></li>
<li><a href="2-1-la-distribución-previa.html#distribuciones-previas-no-informativas"><span class="toc-section-number">2.1.3</span> Distribuciones previas no informativas</a></li>
</ul></li>
<li><a href="2-2-pruebas-de-hipótesis.html#pruebas-de-hipótesis"><span class="toc-section-number">2.2</span> Pruebas de hipótesis</a>
<ul>
<li><a href="2-2-pruebas-de-hipótesis.html#factor-de-bayes"><span class="toc-section-number">2.2.1</span> Factor de Bayes</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#valor-p-bayesiano"><span class="toc-section-number">2.2.2</span> Valor-<span class="math inline">\(p\)</span> Bayesiano</a></li>
</ul></li>
<li><a href="2-3-criterios-de-información.html#criterios-de-información"><span class="toc-section-number">2.3</span> Criterios de información</a>
<ul>
<li><a href="2-3-criterios-de-información.html#criterio-dic"><span class="toc-section-number">2.3.1</span> Criterio DIC</a></li>
<li><a href="2-3-criterios-de-información.html#criterios-aic-y-bic"><span class="toc-section-number">2.3.2</span> Criterios AIC y BIC</a></li>
</ul></li>
</ul></li>
<li><a href="3-modelos-uniparamétricos.html#modelos-uniparamétricos"><span class="toc-section-number">3</span> Modelos uniparamétricos</a>
<ul>
<li><a href="3-1-modelo-bernoulli.html#modelo-bernoulli"><span class="toc-section-number">3.1</span> Modelo Bernoulli</a></li>
<li><a href="3-2-modelo-binomial.html#modelo-binomial"><span class="toc-section-number">3.2</span> Modelo Binomial</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li><a href="A-elementos-de-probabilidad.html#elementos-de-probabilidad"><span class="toc-section-number">A</span> Elementos de probabilidad</a>
<ul>
<li><a href="A-1-distribuciones-discretas.html#distribuciones-discretas"><span class="toc-section-number">A.1</span> Distribuciones discretas</a>
<ul>
<li><a href="A-1-distribuciones-discretas.html#distribución-uniforme-discreta"><span class="toc-section-number">A.1.1</span> Distribución uniforme discreta</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-hipergeométrica"><span class="toc-section-number">A.1.2</span> Distribución hipergeométrica</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-bernoulli"><span class="toc-section-number">A.1.3</span> Distribución Bernoulli</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-binomial"><span class="toc-section-number">A.1.4</span> Distribución binomial</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-binomial-negativa"><span class="toc-section-number">A.1.5</span> Distribución Binomial negativa</a></li>
<li><a href="A-1-distribuciones-discretas.html#distribución-de-poisson"><span class="toc-section-number">A.1.6</span> Distribución de Poisson</a></li>
</ul></li>
<li><a href="A-2-distribuciones-continuas.html#distribuciones-continuas"><span class="toc-section-number">A.2</span> Distribuciones continuas</a>
<ul>
<li><a href="A-2-distribuciones-continuas.html#distribución-uniforme-continua"><span class="toc-section-number">A.2.1</span> Distribución Uniforme Continua</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-weibull"><span class="toc-section-number">A.2.2</span> Distribución Weibull</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-valor-extremo"><span class="toc-section-number">A.2.3</span> Distribución valor-extremo</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-gamma"><span class="toc-section-number">A.2.4</span> Distribución Gamma</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-gamma-inversa"><span class="toc-section-number">A.2.5</span> Distribución Gamma-inversa</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-exponencial"><span class="toc-section-number">A.2.6</span> Distribución exponencial</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-beta"><span class="toc-section-number">A.2.7</span> Distribución Beta</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-normal"><span class="toc-section-number">A.2.8</span> Distribución normal</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-log-normal"><span class="toc-section-number">A.2.9</span> Distribución log-normal</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-ji-cuadrado"><span class="toc-section-number">A.2.10</span> Distribución Ji-cuadrado</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-t-student"><span class="toc-section-number">A.2.11</span> Distribución t-student</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-t-student-generalizada"><span class="toc-section-number">A.2.12</span> Distribución t-student generalizada</a></li>
<li><a href="A-2-distribuciones-continuas.html#distribución-f"><span class="toc-section-number">A.2.13</span> Distribución F</a></li>
</ul></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribuciones-multivariadas"><span class="toc-section-number">A.3</span> Distribuciones multivariadas</a>
<ul>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-multinomial"><span class="toc-section-number">A.3.1</span> Distribución Multinomial</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-dirichelt"><span class="toc-section-number">A.3.2</span> Distribución Dirichelt</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-normal-multivariante"><span class="toc-section-number">A.3.3</span> Distribución Normal Multivariante</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-wishart"><span class="toc-section-number">A.3.4</span> Distribución Wishart</a></li>
<li><a href="A-3-distribuciones-multivariadas.html#distribución-inversa-wishart"><span class="toc-section-number">A.3.5</span> Distribución inversa-Wishart</a></li>
</ul></li>
</ul></li>
<li><a href="B-matriz-de-información.html#matriz-de-información"><span class="toc-section-number">B</span> Matriz de información</a></li>
<li><a href="C-elementos-de-simulación-estadística.html#elementos-de-simulación-estadística"><span class="toc-section-number">C</span> Elementos de simulación estadística</a>
<ul>
<li><a href="C-1-métodos-directos.html#métodos-directos"><span class="toc-section-number">C.1</span> Métodos directos</a>
<ul>
<li><a href="C-1-métodos-directos.html#método-de-la-transformación-uniforme"><span class="toc-section-number">C.1.1</span> Método de la transformación uniforme</a></li>
<li><a href="C-1-métodos-directos.html#método-de-la-grilla"><span class="toc-section-number">C.1.2</span> Método de la grilla</a></li>
</ul></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#métodos-de-monte-carlo-vía-cadenas-de-markov"><span class="toc-section-number">C.2</span> Métodos de Monte Carlo vía cadenas de Markov</a>
<ul>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#el-muestreador-de-gibbs"><span class="toc-section-number">C.2.1</span> El muestreador de Gibbs</a></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#el-algoritmo-de-metrópolis-hastings"><span class="toc-section-number">C.2.2</span> El algoritmo de Metrópolis-Hastings</a></li>
<li><a href="C-2-métodos-de-monte-carlo-vía-cadenas-de-markov.html#buenas-prácticas-en-la-aplicación-de-métodos-mcmc"><span class="toc-section-number">C.2.3</span> Buenas prácticas en la aplicación de métodos MCMC</a></li>
</ul></li>
</ul></li>
<li><a href="referencias.html#referencias">Referencias</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="distribuciones-multivariadas" class="section level2" number="4.3">
<h2><span class="header-section-number">A.3</span> Distribuciones multivariadas</h2>
<div id="distribución-multinomial" class="section level3" number="4.3.1">
<h3><span class="header-section-number">A.3.1</span> Distribución Multinomial</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-51" class="definition"><strong>Definición A.22  </strong></span>Un vector aleatorio <span class="math inline">\(\mathbf{Y}=(Y_1,\ldots,Y_p&#39;)\)</span> tiene distribución multinomial si su función de densidad está dada por:</p>
<p><span class="math display">\[\begin{equation}
p(\mathbf{Y} \mid \boldsymbol \theta)=\binom{n}{y_1,\ldots,y_p}\theta_1^{y_1}\cdots\theta_p^{y_p} \ \ \ \ \ \theta_i&gt;0 \texttt{, } \sum_{i=1}^p\theta_i=1  \texttt{ y } \sum_{i=1}^py_i=p
\end{equation}\]</span></p>
<p>donde</p>
<span class="math display">\[\begin{equation}
\binom{p}{y_1,\ldots,y_p}=\frac{p!}{y_1!\cdots y_p!}.
\end{equation}\]</span>
</div>
<p>Como <span class="citation"><a href="#ref-Gelman03" role="doc-biblioref">A. Gelman et al.</a> (<a href="#ref-Gelman03" role="doc-biblioref">2003</a>)</span>, afirma esta distribución es una generalización de la distribución binomial. La distribución marginal de una sola variable <span class="math inline">\(Y_i\)</span> es <span class="math inline">\(Binomial(p,\theta_i)\)</span></p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-52" class="proposition"><strong>Resultado A.27  </strong></span>Si <span class="math inline">\(\mathbf{Y}\)</span> es una vector aleatorio con distribución multinomial, entonces</p>
<ul>
<li><span class="math inline">\(E(\mathbf{Y})=p(\theta_1,\ldots,\theta_p)&#39;\)</span>.</li>
<li><span class="math inline">\(Var(\mathbf{Y})_{ij}=  \begin{cases}  p\theta_i(1-\theta_i) &amp; \text{si $i=j$}\\  -p\theta_i\theta_j &amp; \text{si $i\neq j$}  \end{cases}\)</span></li>
</ul>
</div>
</div>
<div id="distribución-dirichelt" class="section level3" number="4.3.2">
<h3><span class="header-section-number">A.3.2</span> Distribución Dirichelt</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-53" class="definition"><strong>Definición A.23  </strong></span>Un vector aleatorio <span class="math inline">\(\mathbf{Y}=(Y_1,\ldots,Y_p&#39;)\)</span> tiene distribución Dirichelt si su función de densidad está dada por:</p>
<span class="math display">\[\begin{equation}
p(\mathbf{Y} \mid \boldsymbol \theta)=\frac{\Gamma(\theta_1+\cdots+\theta_p)}{\Gamma(\theta_1)\cdots\Gamma(\theta_p)}
y^{\theta_1-1}\cdots y^{\theta_p-1} \ \ \ \ \ \theta_i&gt;0 \texttt{ y } \sum_{i=1}^p\theta_i=1.
\end{equation}\]</span>
</div>
<p>Esta distribución es una generalización de la distribución beta. La distribución marginal de una sola variable <span class="math inline">\(Y_i\)</span> es <span class="math inline">\(Beta(\theta_i,(\sum_{i=1}^p\theta_i)-\theta_i)\)</span></p>

<div class="proposition">
<p><span id="prp:unnamed-chunk-54" class="proposition"><strong>Resultado A.28  </strong></span>Si <span class="math inline">\(\mathbf{Y}\)</span> es una vector aleatorio con distribución Dirichlet, entonces</p>
<ul>
<li><span class="math inline">\(E(\mathbf{Y})=(\sum_{i=1}^p\theta_i)^{-1}(\theta_1,\ldots,\theta_p)&#39;\)</span>.</li>
<li><span class="math inline">\(Var(\mathbf{Y})_{ij}= \begin{cases} \frac{\theta_i(\sum_{i=1}^p\theta_i-\theta_i)}{(\sum_{i=1}^p\theta_i)^2(\sum_{i=1}^p\theta_i+1)} &amp; \text{si $i=j$}\\ -\frac{\theta_i\theta_j)}{(\sum_{i=1}^p\theta_i)^2(\sum_{i=1}^p\theta_i+1)} &amp; \text{si $i\neq j$}  \end{cases}\)</span>
</div></li>
</ul>
</div>
<div id="distribución-normal-multivariante" class="section level3" number="4.3.3">
<h3><span class="header-section-number">A.3.3</span> Distribución Normal Multivariante</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-55" class="definition"><strong>Definición A.24  </strong></span>Un vector aleatorio <span class="math inline">\(\mathbf{Y}=(Y_1,\ldots,Y_p&#39;)\)</span> tiene distribución normal multivariante de orden <span class="math inline">\(p\)</span>, denotada como <span class="math inline">\(\mathbf{Y}\sim N_p(\boldsymbol \theta,\boldsymbol \Sigma)\)</span>, si su función de densidad está dada por:</p>
<p><span class="math display">\[\begin{equation}
p(\mathbf{Y} \mid \boldsymbol \theta,\boldsymbol \Sigma)=(2\pi)^{-p/2} \mid \boldsymbol \Sigma\mid ^{-1/2}
\exp\left\{-\frac{1}{2}(\mathbf{y}-\boldsymbol \theta)&#39;\boldsymbol \Sigma(\mathbf{y}-\boldsymbol \theta)\right\}
\end{equation}\]</span></p>
donde <span class="math inline">\(\mid \boldsymbol \Sigma\mid\)</span> se refiere al determinante de la matriz <span class="math inline">\(\boldsymbol \Sigma\)</span>, la cual es simétrica y definida positiva de orden <span class="math inline">\(p\times p\)</span>.
</div>
<p><br></p>
<p>La distribución Normal Multivariante es el baluarte de una gran cantidad de técnicas y métodos estadísticos como son los modelos lineales, los modelos lineales generalizados, el análisis factorial, etc. Algunas de sus propiedades se citan a continuación.</p>

<div class="proposition">
<p><span id="prp:normalmulti" class="proposition"><strong>Resultado A.29  </strong></span>Si <span class="math inline">\(\mathbf{Y}=(Y_1,\ldots,Y_p&#39;)\)</span> es una vector aleatorio con distribución normal multivariante, entonces</p>
<ul>
<li>La distribución marginal de cualquier subconjunto de componentes de <span class="math inline">\(\mathbf{Y}\)</span> es también normal multivariante. Por ejemplo si <span class="math inline">\(\mathbf{Y}\)</span> es particionado en <span class="math inline">\(\mathbf{Y}=(\mathbf{Y}_1&#39;,\mathbf{Y}_2&#39;)\)</span>, entonces <span class="math inline">\(p(\mathbf{Y}_1)\)</span> seguiría una distribución normal multivariante, al igual que <span class="math inline">\(p(\mathbf{Y}_2)\)</span>.</li>
<li>Cualquier transformación lineal de <span class="math inline">\(\mathbf{Y}\)</span> es normal multivariante y su dimensión equivale al rango de la transformación. en particular, la suma de las componentes del vector, dada por <span class="math inline">\(\sum_{i=1}^pY_i\)</span> sigue una distribución normal univariada.</li>
<li>La distribución condicional de <span class="math inline">\(\mathbf{Y}\)</span>, restringida a un subespacio lineal es normal.</li>
<li>La distribución condicional de cualquier sub-vector de elementos de <span class="math inline">\(\mathbf{Y}\)</span> dados los restantes elementos es normal multivariante. Más aún, si <span class="math inline">\(\mathbf{Y}\)</span> es particionado en <span class="math inline">\(\mathbf{Y}=(\mathbf{Y}_1&#39;,\mathbf{Y}_2&#39;)\)</span>, entonces <span class="math inline">\(p(\mathbf{Y}_1 \mid \mathbf{Y}_2)\)</span> es normal multivariada con
<span class="math display">\[\begin{align*}
E(\mathbf{Y}_1 \mid \mathbf{Y}_2)&amp;=E(\mathbf{Y}_1)+Cov(\mathbf{Y}_1,\mathbf{Y}_2)(Var(\mathbf{Y}_2))^{-1}(\mathbf{Y}_2-E(\mathbf{Y}_2))\\
Var(\mathbf{Y}_1 \mid \mathbf{Y}_2)&amp;=Var(\mathbf{Y}_1)-Cov(\mathbf{Y}_1,\mathbf{Y}_2)(Var(\mathbf{Y}_2))^{-1}Cov(\mathbf{Y}_2,\mathbf{Y}_1)
\end{align*}\]</span></li>
<li>Si <span class="math inline">\(\mathbf{X}\)</span> es un vector con distribución normal multivariante, entonces <span class="math inline">\(\mathbf{X}+\mathbf{Y}\)</span> tiene una distribución normal multivariante. En particular si <span class="math inline">\(\mathbf{X}\)</span> es independiente de <span class="math inline">\(\mathbf{Y}\)</span>, comparten el mismo orden <span class="math inline">\(p\)</span> y <span class="math inline">\(\mathbf{X}\sim N_p(\boldsymbol \mu,\boldsymbol \Gamma)\)</span>, entonces <span class="math inline">\(\mathbf{X}+\mathbf{Y}\sim N_p(\boldsymbol \mu+\boldsymbol \theta,\boldsymbol \Gamma+\boldsymbol \Sigma)\)</span>.</li>
</ul>
</div>

<div class="proposition">
<p><span id="prp:unnamed-chunk-56" class="proposition"><strong>Resultado A.30  </strong></span>Si <span class="math inline">\(\mathbf{Y}\)</span> es una vector aleatorio con distribución Normal Multivariante, entonces</p>
<ul>
<li><span class="math inline">\(E(\mathbf{Y})=\boldsymbol \theta=(\theta_1,\ldots,\theta_n)&#39;\)</span>.</li>
<li><span class="math inline">\(Var(\mathbf{Y})=\boldsymbol \Sigma\)</span></li>
</ul>
</div>

<div class="proposition">
<p><span id="prp:unnamed-chunk-57" class="proposition"><strong>Resultado A.31  </strong></span>Dado <span class="math inline">\(\mathbf{Y}\)</span> un vector aleatorio particionado como <span class="math inline">\(\mathbf{Y}=(\mathbf{Y}_1&#39;,\mathbf{Y}_2&#39;)\)</span> con esperanza <span class="math inline">\(\boldsymbol \theta=(\boldsymbol \theta_1&#39;,\boldsymbol \theta_2&#39;)\)</span> y matrix de varianzas y covarianzas</p>
<p><span class="math display">\[\begin{equation*}
\boldsymbol \Sigma=\begin{pmatrix}
\boldsymbol \Sigma_{11}&amp;\boldsymbol \Sigma_{12}\\
\boldsymbol \Sigma_{21}&amp;\boldsymbol \Sigma_{22}
\end{pmatrix}.
\end{equation*}\]</span></p>
<p>Si <span class="math inline">\(\mathbf{Y}_1 \mid \mathbf{Y}_2\sim N(\boldsymbol \theta_1+\boldsymbol \Sigma_{12}\boldsymbol \Sigma_{22}^{-1}(\mathbf{Y}_2-\boldsymbol \theta_2),\boldsymbol \Sigma_{11}-\boldsymbol \Sigma_{12}\boldsymbol \Sigma_{22}^{-1}\boldsymbol \Sigma_{21})\)</span> y <span class="math inline">\(\mathbf{Y}_2\sim N(\boldsymbol \theta_2,\boldsymbol \Sigma_{22})\)</span>, entonces se tiene que</p>
<span class="math display">\[\begin{equation*}
\mathbf{Y}\sim N(\boldsymbol \theta,\boldsymbol \Sigma).
\end{equation*}\]</span>
</div>

<div class="proposition">
<p><span id="prp:unnamed-chunk-58" class="proposition"><strong>Resultado A.32  </strong></span>Si <span class="math inline">\(\mathbf{Y}_1,\ldots,\mathbf{Y}_n\)</span> es una muestra aleatoria de vectores con distribución Normal Multivariante, entonces la verosimilitud de la muestra se puede escribir como</p>
<p><span class="math display">\[\begin{equation}
\prod_{i=1}^np(\mathbf{Y}_i \mid \boldsymbol \theta,\boldsymbol \Sigma)\propto \mid \boldsymbol \Sigma\mid ^{-n/2}\exp\left\{-\frac{1}{2}traza(\boldsymbol \Sigma^{-1}\mathbf{S}_{\boldsymbol \theta})\right\}
\end{equation}\]</span></p>
Donde <span class="math inline">\(\mathbf{S}_{\boldsymbol \theta}=\sum_{i=1}^n(\mathbf{Y}_i-\boldsymbol \theta)(\mathbf{Y}_i-\boldsymbol \theta)&#39;\)</span>.
</div>
<p><br></p>

<div class="proof">
<p> <span class="proof"><em>Prueba. </em></span> La verosimilitud de la muestra aleatoria está dada por</p>
<p><span class="math display">\[\begin{align*}
\prod_{i=1}^np(\mathbf{Y}_i \mid \boldsymbol \theta,\boldsymbol \Sigma)
&amp;\propto \mid \boldsymbol \Sigma\mid ^{-n/2}\exp\left\{-\frac{1}{2}\sum_{i=1}^n
(\mathbf{Y}_i-\boldsymbol \theta)&#39;\boldsymbol \Sigma^{-1}(\mathbf{Y}_i-\boldsymbol \theta)\right\}\\
&amp;= \mid \boldsymbol \Sigma\mid ^{-n/2}\exp\left\{-\frac{1}{2}traza\left(\boldsymbol \Sigma^{-1}\mathbf{S}_{\boldsymbol \theta}\right)\right\}
\end{align*}\]</span></p>
<p>Puesto que, por las propiedades del operador <span class="math inline">\(traza\)</span>, se tiene que</p>
<ul>
<li>Si <span class="math inline">\(c\)</span> es un escalar, entonces <span class="math inline">\(c=traza(c)\)</span>.</li>
<li>Si <span class="math inline">\(\mathbf{A}\)</span> y <span class="math inline">\(\mathbf{B}\)</span> son dos matrices, entonces <span class="math inline">\(traza(\mathbf{AB})=traza(\mathbf{BA})\)</span></li>
<li>Si <span class="math inline">\(\mathbf{A}_i\)</span> (i=1,,n) son matrices del mismo tamaño, entonces <span class="math inline">\(\sum_{i=1}^ntraza(\mathbf{A}_i)=traza\left(\sum_{i=1}^n\mathbf{A}_i\right)\)</span></li>
</ul>
<p>Por lo anterior,</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^n(\mathbf{Y}_i-\boldsymbol \theta)&#39;\boldsymbol \Sigma^{-1}(\mathbf{Y}_i-\boldsymbol \theta)
&amp;=traza\left[\sum_{i=1}^n(\mathbf{Y}_i-\boldsymbol \theta)&#39;\boldsymbol \Sigma^{-1}(\mathbf{Y}_i-\boldsymbol \theta)\right]\\
&amp;=\sum_{i=1}^ntraza[\boldsymbol \Sigma^{-1}(\mathbf{Y}_i-\boldsymbol \theta)(\mathbf{Y}_i-\boldsymbol \theta)&#39;)]\\
&amp;=traza\left[\boldsymbol \Sigma^{-1}\sum_{i=1}^n(\mathbf{Y}_i-\boldsymbol \theta)(\mathbf{Y}_i-\boldsymbol \theta)&#39;)\right]\\
&amp;=traza(\boldsymbol \Sigma^{-1}\mathbf{S}_{\boldsymbol \theta})
\end{align*}\]</span>
</div>
</div>
<div id="distribución-wishart" class="section level3" number="4.3.4">
<h3><span class="header-section-number">A.3.4</span> Distribución Wishart</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-60" class="definition"><strong>Definición A.25  </strong></span>Sea <span class="math inline">\(\boldsymbol \Sigma\)</span> una matriz aleatoria simétrica y definida positiva de tamaño <span class="math inline">\(p\times p\)</span>. Se dice que <span class="math inline">\(\boldsymbol \Sigma\)</span> tiene distribución Wishart con <span class="math inline">\(v\)</span> grados de libertad, denotada como <span class="math inline">\(\mathbf{Y}\sim Wishart_v(\boldsymbol \Lambda)\)</span>, si su función de densidad está dada por:</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol \Sigma)&amp;=\left( 2^{vp/2}\pi^{p(p-1)/4}\prod_{i=1}^p \Gamma\left(\frac{v+1-i}{2}\right)    \right)^{-1} \notag \\
&amp;\hspace{2cm}\times
 \mid \boldsymbol \Lambda\mid ^{-v/2} \mid \boldsymbol \Sigma\mid ^{(v-p-1)/2}
\exp\left\{ -\frac{1}{2}traza(\boldsymbol \Lambda^{-1}\boldsymbol \Sigma)\right\}
\end{align}\]</span></p>
donde <span class="math inline">\(\mid \boldsymbol \Lambda\mid\)</span> se refiere al determinante de la matriz <span class="math inline">\(\boldsymbol \Lambda\)</span>, la cual es simétrica y definida positiva de orden <span class="math inline">\(p\times p\)</span>.
</div>

<div class="proposition">
<span id="prp:unnamed-chunk-61" class="proposition"><strong>Resultado A.33  </strong></span>Si <span class="math inline">\(\boldsymbol \Sigma\)</span> es una matriz aleatoria con distribución Wishart con <span class="math inline">\(v\)</span> grados de libertad, entonces <span class="math inline">\(E(\boldsymbol \Sigma)=v\boldsymbol \Lambda\)</span>
</div>
</div>
<div id="distribución-inversa-wishart" class="section level3" number="4.3.5">
<h3><span class="header-section-number">A.3.5</span> Distribución inversa-Wishart</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-62" class="definition"><strong>Definición A.26  </strong></span>Sea <span class="math inline">\(\boldsymbol \Sigma\)</span> una matriz aleatoria simétrica y definida positiva de tamaño <span class="math inline">\(p\times p\)</span>. Se dice que <span class="math inline">\(\boldsymbol \Sigma\)</span> tiene distribución Wishart con <span class="math inline">\(v\)</span> grados de libertad, denotada como <span class="math inline">\(\mathbf{Y}\sim Wishart_v(\boldsymbol \Lambda)\)</span>, si su función de densidad está dada por:</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol \Sigma)&amp;=\left( 2^{vp/2}\pi^{p(p-1)/4}\prod_{i=1}^p \Gamma\left(\frac{v+1-i}{2}\right)    \right)^{-1} \notag \\
&amp;\hspace{2cm}\times
 \mid \boldsymbol \Lambda\mid ^{v/2} \mid \boldsymbol \Sigma\mid ^{-(v+p+1)/2}
\exp\left\{ -\frac{1}{2}traza(\boldsymbol \Lambda\boldsymbol \Sigma^{-1})\right\}
\end{align}\]</span></p>
donde <span class="math inline">\(\mid \boldsymbol \Lambda\mid\)</span> se refiere al determinante de la matriz <span class="math inline">\(\boldsymbol \Lambda\)</span>, la cual es simétrica y definida positiva de orden <span class="math inline">\(p\times p\)</span>.
</div>

<div class="proposition">
<span id="prp:unnamed-chunk-63" class="proposition"><strong>Resultado A.34  </strong></span>Si <span class="math inline">\(\boldsymbol \Sigma\)</span> es una matriz aleatoria con distribución inversa-Wishart con <span class="math inline">\(v\)</span> grados de libertad, entonces <span class="math inline">\(E(\boldsymbol \Sigma)=\dfrac{1}{v-p-1}\boldsymbol \Lambda\)</span>
</div>

<div class="proposition">
<span id="prp:unnamed-chunk-64" class="proposition"><strong>Resultado A.35  </strong></span>Si <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> es una matriz aleatoria con distribución inversa-Wishart, entonces con <span class="math inline">\(\boldsymbol \Sigma\)</span> tiene distribución Wishart.
</div>

</div>
</div>
<!-- </div> -->
<h3>Referencias</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Gelman03" class="csl-entry">
———. 2003. <em>Bayesian Data Analysis</em>. 2.ª ed. Chapman; Hall/CRC.
</div>
</div>
<p style="text-align: center;">
<a href="A-2-distribuciones-continuas.html"><button class="btn btn-default">Previous</button></a>
<a href="B-matriz-de-información.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
