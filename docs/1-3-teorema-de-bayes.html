<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.3 Teorema de Bayes | Modelos Bayesianos con R y STAN" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN." />
<meta name="github-repo" content="psirusteam/bookdownBayesiano" />

<meta name="author" content="Andrés Gutiérrez - Hanwen Zhang" />

<meta name="date" content="2021-05-30" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Este es el repositorio del libro Modelos Bayesianos con R y STAN.">

<title>1.3 Teorema de Bayes | Modelos Bayesianos con R y STAN</title>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />





</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#prefacio">Prefacio</a></li>
<li><a href="1-tópicos-básicos.html#tópicos-básicos"><span class="toc-section-number">1</span> Tópicos básicos</a>
<ul>
<li><a href="1-1-teoría-de-la-decisión.html#teoría-de-la-decisión"><span class="toc-section-number">1.1</span> Teoría de la decisión</a></li>
<li><a href="1-2-algunos-resultados-de-probabilidad.html#algunos-resultados-de-probabilidad"><span class="toc-section-number">1.2</span> Algunos resultados de probabilidad</a></li>
<li><a href="1-3-teorema-de-bayes.html#teorema-de-bayes"><span class="toc-section-number">1.3</span> Teorema de Bayes</a></li>
</ul></li>
<li><a href="2-inferencia-bayesiana.html#inferencia-bayesiana"><span class="toc-section-number">2</span> Inferencia bayesiana</a>
<ul>
<li><a href="2-1-información-previa.html#información-previa"><span class="toc-section-number">2.1</span> Información previa</a>
<ul>
<li><a href="2-1-información-previa.html#distribuciones-conjugadas"><span class="toc-section-number">2.1.1</span> Distribuciones conjugadas</a></li>
<li><a href="2-1-información-previa.html#familia-exponencial"><span class="toc-section-number">2.1.2</span> Familia exponencial</a></li>
<li><a href="2-1-información-previa.html#distribuciones-previas-no-informativas"><span class="toc-section-number">2.1.3</span> Distribuciones previas no informativas</a></li>
</ul></li>
<li><a href="2-2-pruebas-de-hipótesis.html#pruebas-de-hipótesis"><span class="toc-section-number">2.2</span> Pruebas de hipótesis</a>
<ul>
<li><a href="2-2-pruebas-de-hipótesis.html#factor-de-bayes"><span class="toc-section-number">2.2.1</span> Factor de Bayes</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#valor-p-bayesiano"><span class="toc-section-number">2.2.2</span> Valor <span class="math inline">\(p\)</span> Bayesiano</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#criterio-dic"><span class="toc-section-number">2.2.3</span> Criterio DIC</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#criterio-aic-y-bic"><span class="toc-section-number">2.2.4</span> Criterio AIC y BIC</a></li>
<li><a href="2-2-pruebas-de-hipótesis.html#acerca-de-la-notación"><span class="toc-section-number">2.2.5</span> Acerca de la notación</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="teorema-de-bayes" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Teorema de Bayes</h2>
<p>Desde la revolución estadística de Pearson y Fisher, la inferencia
estadística busca encontrar los valores que parametrizan a la
distribución desconocida de los datos. El primer enfoque, propuesto por
Pearson, afirmaba que si era posible observar a la variable de interés
en todos y cada uno de los individuos de una población, entonces era
posible calcular los parámetros de la distribución de la variable de
interés; por otro lado, si solo se tenía acceso a una muestra
representativa, entonces era posible calcular una estimación de tales
parámetros. Sin embargo, Fisher discrepó de tales argumentos, asumiendo
que las observaciones están sujetas a un error de medición y por lo
tanto, así se tuviese acceso a toda la población, sería imposible calcular
los parámetros de la distribución de la variable de interés.</p>
<p>Del planteamiento de Fisher resultaron una multitud de métodos
estadísticos para la estimación de los parámetros poblacionales. Es
decir, si la distribución de <span class="math inline">\(\mathbf{Y}\)</span> está parametrizada por
<span class="math inline">\(\boldsymbol \theta=(\theta_1,\ldots,\theta_K)\)</span>, <span class="math inline">\(\boldsymbol \theta\in \Theta\)</span> con <span class="math inline">\(\Theta\)</span>
el espacio paramétrico inducido por el comportamiento de la variable de
interés, el objetivo de la teoría estadística inferencial es calcular
una estimación <span class="math inline">\(\hat{\boldsymbol \theta}\)</span> del parámetro <span class="math inline">\(\boldsymbol \theta\)</span>, por medio de los
datos observados. En este enfoque, los parámetros se consideran
cantidades fijas y constantes. Sin embargo, en la última mitad del siglo
XX, algunos investigadores estadísticos comenzaron a reflexionar acerca
de la naturaleza de <span class="math inline">\(\boldsymbol \theta\)</span> y enfocaron la inferencia estadística de
una manera distinta: <em>asumiendo que la distribución de la variable de
interés está condicionada a valores específicos de los parámetros</em>. Es
decir, en términos de notación, si la variable de interés es
<span class="math inline">\(\mathbf{Y}\)</span>, su distribución condicionada a los parámetros toma la
siguiente forma <span class="math inline">\(p(\mathbf{Y} \mid \boldsymbol \theta)\)</span>. Esto implica claramente
que en este nuevo enfoque la naturaleza de los parámetros no es
constante.</p>
<p>En términos de inferencia para <span class="math inline">\(\boldsymbol \theta\)</span>, es necesario encontrar la
distribución de los parámetros condicionada a la observación de los
datos. Para este fin, es necesario definir la distribución conjunta de
la variable de interés con el vector de parámetros.
<span class="math display">\[\begin{equation*}
p(\boldsymbol \theta,\mathbf{Y})=p(\boldsymbol \theta)p(\mathbf{Y} \mid \boldsymbol \theta)
\end{equation*}\]</span></p>
<p>A la distribución <span class="math inline">\(p(\boldsymbol \theta)\)</span> se le conoce con el nombre de
distribución <em>previa</em> y en ella se enmarcan todas y cada una de las
creencias que se tienen acerca del comportamiento estocástico del vector
de parámetros antes de que ocurra la recolección de los datos; <span class="math inline">\(p(\mathbf{Y} \mid \boldsymbol \theta)\)</span> es la distribución de muestreo,
verosimilitud o distribución de los datos. Por otro lado, la
distribución del vector de parámetros condicionada a los datos
observados está dada por</p>
<p><span class="math display" id="eq:Bayes">\[\begin{equation}
\tag{1.1}
p(\boldsymbol \theta\mid \mathbf{Y})=\frac{p(\boldsymbol \theta,\mathbf{Y})}{p(\mathbf{Y})}=\frac{p(\boldsymbol \theta)p(\mathbf{Y} \mid \boldsymbol \theta)}{p(\mathbf{Y})}
\end{equation}\]</span></p>
<p>A la distribución <span class="math inline">\(p(\boldsymbol \theta\mid \mathbf{Y})\)</span> se le conoce con el
nombre de distribución <em>posterior</em> y en ella se enmarcan las
creencias actualizadas acerca del comportamiento estocástico del vector
de parámetros teniendo en cuenta los datos observados <span class="math inline">\(\mathbf{Y}\)</span>.
Nótese que la expresión <a href="1-3-teorema-de-bayes.html#eq:Bayes">(1.1)</a> se compone de una fracción cuyo
denominador no depende del vector de parámetros y considerando a los
datos observados como fijos, corresponde a una constante y puede ser
obviada. Por lo tanto, otra representación de la regla de Bayes está
dada por</p>
<p><span class="math display" id="eq:Bayes1">\[\begin{align}
\tag{1.2}
p(\boldsymbol \theta\mid \mathbf{Y})\propto p(\mathbf{Y} \mid \boldsymbol \theta)p(\boldsymbol \theta)
\end{align}\]</span></p>
<p><span class="citation"><label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">Gelman et al. (2003)<span class="marginnote">Gelman, A., J. B. Carlin, H. S. Stern, y D. B. Rubin. 1995. <em>Bayesian Data Analysis</em>. 1.ª ed. Chapman; Hall/CRC. 2003. <em>Bayesian Data Analysis</em>. 2.ª ed. Chapman; Hall/CRC.</span></span> menciona que esta expresión se conoce como la
distribución <em>posterior no-normalizada</em> y encierra el núcleo
técnico de la inferencia bayesiana. La constante <span class="math inline">\(p(\mathbf{Y})\)</span>
faltante en la expresión <a href="1-3-teorema-de-bayes.html#eq:Bayes1">(1.2)</a> se da a continuación.</p>

<div class="proposition">
<span id="prp:unnamed-chunk-12" class="proposition"><strong>Resultado 1.4  </strong></span>La expresión <span class="math inline">\(p(\mathbf{Y})\)</span> corresponde a una constante <span class="math inline">\(k\)</span> tal que
<span class="math display">\[\begin{equation*}
k=p(\mathbf{Y})=E_{\boldsymbol \theta}[p(Y \mid \boldsymbol \theta)]
\end{equation*}\]</span>
</div>

<div class="proof">
 <span class="proof"><em>Prueba. </em></span> Nótese que
<span class="math display">\[\begin{equation*}
k=p(\mathbf{Y})=\int p(\mathbf{Y},\boldsymbol \theta)\ d\boldsymbol \theta=\int p(\boldsymbol \theta)p(\mathbf{Y} \mid \boldsymbol \theta)\ d\boldsymbol \theta.
\end{equation*}\]</span>
entonces
<span class="math display">\[\begin{align*}
k&amp;=\int p(\mathbf{Y} \mid \boldsymbol \theta)p(\boldsymbol \theta)\ d\boldsymbol \theta\\
&amp;=E_{\boldsymbol \theta}[p(Y \mid \boldsymbol \theta)]
\end{align*}\]</span>
</div>
<p>Curiosamente, el reverendo Thomas Bayes nunca publicó este resultado,
sino que después de su fallecimiento, su amigo el filósofo Richard
Price, encontró los escritos dentro de sus pertenencias, y éstos fueron
publicados en el 1764 en
<em>Philosophical Transactions of the Royal Society of London</em>. Aunque
el teorema de Bayes fue nombrado en honor de Thomas Bayes, es casi
seguro que él mismo no sospechaba del gran impacto de su resultado. De hecho, aproximadamente una década más tarde, Pierre-Simon Laplace también descrubrió el mismo principio, y dedicó gran parte de su vida extendiéndolo y formalizándolo. Más aún, él analizó grandes volumenes de datos relacionados a los nacimientos en diferentes paises para confirmar esta teoría, y sentó las bases de la estadística bayesiana.</p>
<p>A continuación se presenta un ejemplo simple de este sencillo pero
poderoso teorema.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-14" class="example"><strong>Ejemplo 1.4  </strong></span>Suponga que una fábrica del sector industrial produce bolígrafos y que la producción está a cargo de tres máquinas. La primera máquina produce el 50% del total de bolígrafos en el año, la segunda máquina produce el 30% y la última maquina produce el restante 20%. Por supuesto, esta producción esta sujeta al error y por tanto, basados en la experiencia, es posible reconocer que, de los artículos producidos por la primera máquina, el 5% resultan defectuosos; de los artículos producidos por la segunda máquina, el 2% resultan defectuosos y, de los artículos producidos por la última máquina, el 6% resultan defectuosos.</p>
<p>FIGURA - Fabrica1.pdf</p>
<p>Una pregunta natural que surge es acerca de la probabilidad de selección de un artículo defectuoso y para responder a esta pregunta con rigurosidad de probabilística es necesario enfocar la atención en los tópicos básicos que dejamos atrás. En primer lugar, el experimento en cuestión es la selección de un bolígrafo. Para este experimento, una terna <span class="math inline">\((\Omega, \mathfrak{F}, P)\)</span>,<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> <span class="math inline">\(\Omega\)</span> denota el conjunto de todos lo posibles resultados del experimento, <span class="math inline">\(\mathfrak{F}\)</span> denota una <span class="math inline">\(\sigma\)</span>-álgebra y <span class="math inline">\(P\)</span> hace referencia ana medida de probabilidad propiamente definida.</span> llamada comúnmente espacio de medida o espacio de probabilidad, está dada por</p>
<ol style="list-style-type: decimal">
<li>El espacio muestral: <span class="math inline">\(\Omega=\{\text{defectuoso}, \text{No defectouso}\}\)</span></li>
<li>La <span class="math inline">\(\sigma\)</span>-álgebra: <span class="math inline">\(\mathfrak{F}=\{\Omega, \phi, \{\text{Defectuoso}\}, \{\text{No Defectuoso}\}\}\)</span></li>
<li>La función de probabilidad:
<span class="math display">\[\begin{align*}
  p: \mathfrak{F} &amp;\longrightarrow [0,1]\\
  \Omega &amp;\longrightarrow 1\\
  \phi &amp;\longrightarrow 0\\
  \{Defectuoso\}&amp;\longrightarrow P(D)\\
  \{No Defectuoso\}&amp;\longrightarrow 1-P(D)
  \end{align*}\]</span>
en donde, acudiendo al teorema de probabilidad total, se define
<span class="math display">\[\begin{equation*}
  p(D)=p(D \mid M1)P(M1)+p(D \mid M2)P(M2)+p(D \mid M3)P(M3)
  \end{equation*}\]</span></li>
</ol>
<p>Sin embargo, también es posible plantearse otro tipo de preguntas que sirven para calibrar el proceso de producción de artículos defectuosos. Por ejemplo, cabe preguntarse acerca de la probabilidad de que, habiendo seleccionado un artículo defectuoso, éste provenga de la primera máquina<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> Por supuesto que la pregunta también es válida al indagar por la probabilidad de que habiendo seleccionado un artículo defectuoso, éste provenga de la segunda o tercera máquina.</span>. En esta ocasión, el experimento ha cambiado y ahora se trata de seleccionar un artículo defectuoso y para responder a tal cuestionamiento, se debe establecer rigurosamente el espacio de probabilidad que puede estar dado por</p>
<ol style="list-style-type: decimal">
<li>El espacio muestral: <span class="math inline">\(\Omega=\{M1, M2, M3 \}\)</span></li>
<li>La <span class="math inline">\(\sigma\)</span>-álgebra: <span class="math inline">\(\mathfrak{F}^+=\{\Omega, \phi, \{M1\}, \{M2,M3\}\}\)</span></li>
<li>La función de probabilidad:
<span class="math display">\[\begin{align*}
  p: \mathfrak{F}^+ &amp;\longrightarrow [0,1]\\
  \Omega &amp;\longrightarrow 1\\
  \phi &amp;\longrightarrow 0\\
  \{M1\}&amp;\longrightarrow p(M1 \mid D)\\
  \{M2,M3\}&amp;\longrightarrow 1-p(M1 \mid D)
  \end{align*}\]</span>
en donde, acudiendo a la probabilidad condicional, se define
<span class="math display">\[\begin{equation*}
  p(M1 \mid D)=\frac{p(D \mid M1)P(M1)}{p(D \mid M1)P(M1)+p(D \mid M2)P(M2)+p(D \mid M3)P(M3)}
  \end{equation*}\]</span></li>
</ol>
<p>La anterior función de probabilidad se conoce con el nombre de regla de probabilidad de Bayes y, aparte de ser el baluarte de la mayoría de investigaciones estadísticas que se plantean hoy en día, ha sido la piedra de tropiezo de muchos investigadores radicales que trataron de estigmatizar este enfoque tildando a sus seguidores de mediocres matemáticos y pobres probabilistas afirmando que la regla de probabilidad de Bayes es sólo un artilugio diseñado para divertirse en el tablero.</p>
Pues bien, la interpretación de la regla de bayes se puede realizar en el sentido de actualización de la estructura probabilística que gobierna el experimento. Y esta actualización tiene mucho sentido práctico cuando se cae en la cuenta de que la vida real está llena de calibradores y que las situaciones generadas son consecuencia de algún cambio estructural. De esta forma, el conocimiento de la probabilidad de que el artículo sea producido por la primera máquina se actualiza al conocer que este artículo particular es defectuoso y de esta manera calibra la estructura aleatoria que existe detrás del contexto de la fábrica de bolígrafos. Aparte de servir para resolver problemas como el anteriormente mencionado, la regla de bayes ha marcado el comienzo de un nuevo enfoque de análisis de datos, no solamente porque hace explícitas las relaciones causales entre los procesos aleatorios, sino también porque facilita la inferencia estadística y la interpretación de los resultados.
</div>
<p><br>
En el campo de la medicina, también se ha visto un gran número de la
aplicación del teorema de Bayes. A continuación se enuncia uno de ellos:</p>

<div class="example">
<p><span id="exm:unnamed-chunk-15" class="example"><strong>Ejemplo 1.5  </strong></span>El Grupo de Trabajo de Servicios Preventivos de los Estados Unidos (USPSTF) hizo unas nuevas y controversiales recomendaciones <a href="https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/breast-cancer-screening">recomendaciones</a> sobre la detección del cáncer de mama dentro de los cuales no recomienda el examen de la mamografía en mujeres entre 40 y 49 años de edad, afirmando que la práctica bienal de este examen debe ser una decisión individual según el contexto particular de la paciente. Por otro lado, la USPSTF sí recomienda tal práctica de forma bienal en grupos de mujeres de entre 50 y 74 años de edad, puesto que no encontró suficiente evidencia de beneficio o daño adicional en realizar este examen en mujeres mayores a los 74 años. Además, también recomendó <em>no</em> realizar auto exámanes de senos, contrario a las recomendaciones y consejos que da la mayoría de los profesionales y organizaciones de la salud, incluyendo la <em>Amerian Cancer Society</em>. Como información adicional, se sabe que:</p>
<ul>
<li>Los expertos estiman que un 12.3% de las mujeres desarrollan formas invasivas del cáncer de mama durante la vida.</li>
<li>La probabilidad de que una mujer desarrolle el cáncer de mama entre los 40 y los 49 años de edad es 1 en 69, y esta probabilidad aumenta a medida que envejezca, de tal forma que llega a ser de 1 en 38 en mujeres de entre 50 y 59 años.</li>
<li>El cáncer de mama es más difícil de detectar en mujeres jóvenes puesto que el tejido mamario es más denso y fibroso. Los expertos estiman que la tasa de un falso positivo es de 97.8 por cada 1000 mujeres de 40 y 49 años, y esta tasa disminuye a 86.6 por cada 1000 mujeres entre 50 y 59 años.</li>
<li>La tasa de un falso negativo es de 1 por cada 1000 mujeres de 40 y 49 años, y es de 1.1 por cada 1000 mujeres entre 50 y 59 años.</li>
</ul>
<p>Resumiendo las anteriores afirmaciones, tenemos las siguientes probabilidades</p>
<table>
<thead>
<tr class="header">
<th>Probabilidad</th>
<th>40 - 49</th>
<th>50 - 59 años</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cáncer</td>
<td>1/69=0.01449</td>
<td>1/38=0.02632</td>
</tr>
<tr class="even">
<td>No cáncer</td>
<td>68/69=0.9855</td>
<td>37/38=0.97368</td>
</tr>
<tr class="odd">
<td>Positivo <span class="math inline">\(\mid\)</span> No cáncer</td>
<td>0.0978</td>
<td>0.0866</td>
</tr>
<tr class="even">
<td>Negativo <span class="math inline">\(\mid\)</span> No cáncer</td>
<td>0.9022</td>
<td>0.9134</td>
</tr>
<tr class="odd">
<td>Positivo <span class="math inline">\(\mid\)</span> Cáncer</td>
<td>0.999</td>
<td>0.9989</td>
</tr>
<tr class="even">
<td>Negativo <span class="math inline">\(\mid\)</span> Cáncer</td>
<td>0.001</td>
<td>0.0011</td>
</tr>
</tbody>
</table>
<p>Utilizando la regla de Bayes, se puede calcular las siguientes probabilidades para mujeres de 40 y 49 años:
<span class="math display">\[\begin{align*}
P(\text{Cáncer}|\text{Positivo})&amp;=\frac{P(\text{Positivo}|\text{Cáncer})P(\text{Cáncer})}{P(\text{Positivo}|\text{Cáncer})P(\text{Cáncer})+P(\text{Positivo}|\text{No cáncer})P(\text{No cáncer})}\\
&amp;=\frac{0.999*0.01449}{0.999*0.01449+0.0978*0.9855}\\
&amp;=0.1305
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
P(\text{Cáncer}|\text{Negativo})&amp;=\frac{P(\text{Negativo}|\text{Cáncer})P(\text{Cáncer})}{P(\text{Negativo}|\text{Cáncer})P(\text{Cáncer})+P(\text{Negativo}|\text{No cáncer})P(\text{No cáncer})}\\
&amp;=\frac{0.001*0.01449}{0.001*0.01449+0.9022*0.9855}\\
&amp;=0.0000163
\end{align*}\]</span></p>
<p>Similarmente, se puede calcular estas dos probabilidades para las mujeres de 50 y 59 años.</p>
<table>
<thead>
<tr class="header">
<th>Probabilidad</th>
<th>40 - 49 años</th>
<th>50 - 59 años</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cáncer <span class="math inline">\(\mid\)</span> Positivo</td>
<td>0.1305985</td>
<td>0.23769</td>
</tr>
<tr class="even">
<td>No cáncer <span class="math inline">\(\mid\)</span> Positivo</td>
<td>0.8694223</td>
<td>0.7623123</td>
</tr>
<tr class="odd">
<td>Cáncer <span class="math inline">\(\mid\)</span> Negativo</td>
<td>0.0000163</td>
<td>0.0000326</td>
</tr>
<tr class="even">
<td>No cáncer <span class="math inline">\(\mid\)</span> Negativo</td>
<td>0.9999837</td>
<td>0.9999674</td>
</tr>
</tbody>
</table>
Los anteriores resultados muestran cómo cambia la probabilidad de tener cáncer al condicionar en los resultados de la pruebe. Entre estos valores se puede ver que, con un resultado positivo en el examen, la probabilidad de tener efectivamente el cáncer es aproximadamente diez puntos porcentuales más bajo en mujeres de edad de 40 y 49 años, de donde se puede sustentar la recomendación de no efectuar este examen en mujeres de este rango de edad.
</div>

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="1-2-algunos-resultados-de-probabilidad.html"><button class="btn btn-default">Previous</button></a>
<a href="2-inferencia-bayesiana.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
